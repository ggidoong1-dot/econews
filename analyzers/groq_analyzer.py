"""
Groq AI ê¸°ë°˜ ë‰´ìŠ¤ ë¶„ì„ê¸°
ìƒˆë²½ ê¸€ë¡œë²Œ ë‰´ìŠ¤ë¥¼ ì´ˆê³ ì†ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ í•œêµ­ ì£¼ì‹ì‹œì¥ ì˜í–¥ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

í•µì‹¬ íŠ¹ì§•:
- 2ë‹¨ê³„ ë¶„ì„ íŒŒì´í”„ë¼ì¸ (1ì°¨ í•„í„°ë§ â†’ 2ì°¨ ì‹¬ì¸µë¶„ì„)
- llama-3.1-8b-instant: ë¹ ë¥¸ 1ì°¨ í•„í„°ë§ (560 TPS)
- llama-3.3-70b-versatile: ì‹¬ì¸µ ë¶„ì„ (280 TPS)
"""
import json
import time
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timezone, timedelta
import os

# Groq SDK
try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False
    Groq = None

import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import config

logger = config.setup_logger(__name__)


# ==============================================
# Groq ëª¨ë¸ ì„¤ì •
# ==============================================
GROQ_MODELS = {
    "fast": "llama-3.1-8b-instant",      # 1ì°¨ í•„í„°ë§ìš© (ë¹ ë¥´ê³  ì €ë ´)
    "deep": "llama-3.3-70b-versatile",   # 2ì°¨ ì‹¬ì¸µë¶„ì„ìš© (ì •í™•)
    "tool": "llama-3-groq-70b-tool-use", # ë„êµ¬ ì‚¬ìš© íŠ¹í™”
}

# Rate Limit ì„¤ì • (Groq Free tier: 30 RPM)
RATE_LIMIT_DELAY = {
    "fast": 2.0,   # 8b ëª¨ë¸ì€ ë¹ ë¥´ë¯€ë¡œ 2ì´ˆ ëŒ€ê¸°
    "deep": 4.0,   # 70b ëª¨ë¸ì€ ì¡°ê¸ˆ ë” ì—¬ìœ ìˆê²Œ
}


# ==============================================
# Groq í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# ==============================================
class GroqAnalyzer:
    """Groq AI ë¶„ì„ê¸° í´ë˜ìŠ¤"""
    
    def __init__(self, api_key: str = None):
        """
        Args:
            api_key: Groq API í‚¤ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
        """
        self.api_key = api_key or os.getenv("GROQ_API_KEY")
        self.client = None
        self.available = False
        
        if not GROQ_AVAILABLE:
            logger.warning("âš ï¸ groq íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. pip install groq ì‹¤í–‰ í•„ìš”")
            return
            
        if not self.api_key:
            logger.warning("âš ï¸ GROQ_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            return
        
        try:
            self.client = Groq(api_key=self.api_key)
            self.available = True
            logger.info("âœ… Groq API ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ Groq API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _call_api(self, prompt: str, model_type: str = "fast", 
                  temperature: float = 0.3, max_tokens: int = 1024) -> Optional[str]:
        """
        Groq API í˜¸ì¶œ
        
        Args:
            prompt: í”„ë¡¬í”„íŠ¸
            model_type: "fast" ë˜ëŠ” "deep"
            temperature: ì°½ì˜ì„± (0.0~1.0)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            
        Returns:
            str: ì‘ë‹µ í…ìŠ¤íŠ¸ ë˜ëŠ” None
        """
        if not self.available:
            return None
        
        model = GROQ_MODELS.get(model_type, GROQ_MODELS["fast"])
        
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=[
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ í•œêµ­ ì£¼ì‹ì‹œì¥ ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ê¸€ë¡œë²Œ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ í•œêµ­ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì •í™•íˆ ì˜ˆì¸¡í•©ë‹ˆë‹¤."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=temperature,
                max_tokens=max_tokens,
                response_format={"type": "json_object"}
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"Groq API í˜¸ì¶œ ì‹¤íŒ¨ ({model}): {e}")
            return None
    
    def filter_korea_relevant(self, article: Dict) -> Dict:
        """
        1ì°¨ í•„í„°ë§: í•œêµ­ ì‹œì¥ ê´€ë ¨ì„± ë¹ ë¥´ê²Œ íŒë‹¨
        llama-3.1-8b-instant ì‚¬ìš© (ì´ˆê³ ì†)
        
        Args:
            article: ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„°
            
        Returns:
            Dict: ê´€ë ¨ì„± íŒë‹¨ ê²°ê³¼
        """
        title = article.get('title', '')
        description = article.get('description', '')[:300]
        
        prompt = f"""ë‹¤ìŒ ë‰´ìŠ¤ê°€ í•œêµ­ ì£¼ì‹ì‹œì¥ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ë¹ ë¥´ê²Œ íŒë‹¨í•˜ì„¸ìš”.

ì œëª©: {title}
ë‚´ìš©: {description}

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
{{
    "is_relevant": true/false,
    "relevance_level": "high/medium/low/none",
    "quick_reason": "í•œ ë¬¸ì¥ ì´ìœ "
}}"""

        result = self._call_api(prompt, model_type="fast", max_tokens=256)
        
        if result:
            try:
                return json.loads(result)
            except json.JSONDecodeError:
                pass
        
        # í´ë°±: ê·œì¹™ ê¸°ë°˜ íŒë‹¨
        return self._rule_based_relevance(article)
    
    def analyze_deep(self, article: Dict) -> Dict:
        """
        2ì°¨ ì‹¬ì¸µ ë¶„ì„: ìƒì„¸í•œ ì˜í–¥ ë¶„ì„ ë° ì˜ˆì¸¡
        llama-3.3-70b-versatile ì‚¬ìš© (ì •í™•)
        
        Args:
            article: ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„°
            
        Returns:
            Dict: ìƒì„¸ ë¶„ì„ ê²°ê³¼
        """
        title = article.get('title', '')
        description = article.get('description', '')[:500]
        source = article.get('source', '')
        
        prompt = f"""ê¸€ë¡œë²Œ ë‰´ìŠ¤ì˜ í•œêµ­ ì£¼ì‹ì‹œì¥ ì˜í–¥ì„ ìƒì„¸íˆ ë¶„ì„í•˜ì„¸ìš”.

[ë‰´ìŠ¤ ì •ë³´]
ì œëª©: {title}
ë‚´ìš©: {description}
ì¶œì²˜: {source}

[ë¶„ì„ ìš”ì²­]
JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
{{
    "korea_relevance": "high/medium/low/none",
    "impact_direction": "positive/negative/neutral",
    "confidence": 0.0-1.0,
    "affected_sectors": ["ì„¹í„°ëª…1", "ì„¹í„°ëª…2"],
    "impact_timing": "ì‹œì´ˆê°€/ì¥ì¤‘/ì¥ë§ˆê°/ë‹¤ìŒë‚ ",
    "investment_strategy": "ë§¤ìˆ˜/ë§¤ë„/ê´€ë§",
    "reasoning": "í•œêµ­ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ì„¤ëª… (í•œê¸€, 150ì ì´ë‚´)",
    "title_ko": "í•œê¸€ ì œëª© ë²ˆì—­",
    "key_factors": ["í•µì‹¬ ìš”ì¸1", "í•µì‹¬ ìš”ì¸2"]
}}

[ì„¹í„° ì„ íƒì§€]
ë°˜ë„ì²´, 2ì°¨ì „ì§€, ìë™ì°¨, ë°”ì´ì˜¤, IT/ì¸í„°ë„·, ê¸ˆìœµ, ì¡°ì„ , í™”í•™/ì •ìœ , ì² ê°•, ë°©ì‚°, ì›ìë ¥, ì—”í„°, ê±´ì„¤, ìœ í†µ, í†µì‹ 

[ì£¼ì˜ì‚¬í•­]
- korea_relevanceê°€ "none"ì´ë©´ ë‹¤ë¥¸ í•„ë“œëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ
- confidenceëŠ” í™•ì‹ ì´ ì—†ìœ¼ë©´ ë‚®ê²Œ ì„¤ì •
- investment_strategyëŠ” ì‹ ì¤‘í•˜ê²Œ ì œì•ˆ"""

        result = self._call_api(prompt, model_type="deep", max_tokens=1024)
        
        if result:
            try:
                parsed = json.loads(result)
                parsed["analysis_method"] = "groq_deep"
                return parsed
            except json.JSONDecodeError as e:
                logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        # í´ë°±: ê·œì¹™ ê¸°ë°˜ ë¶„ì„
        return self._rule_based_analysis(article)
    
    def _rule_based_relevance(self, article: Dict) -> Dict:
        """ê·œì¹™ ê¸°ë°˜ ê´€ë ¨ì„± íŒë‹¨ (í´ë°±)"""
        title = article.get('title', '').lower()
        desc = article.get('description', '').lower()
        text = f"{title} {desc}"
        
        # ì§ì ‘ ê´€ë ¨ í‚¤ì›Œë“œ
        direct_keywords = [
            "korea", "korean", "samsung", "sk hynix", "hyundai", "kia",
            "lg", "kospi", "kosdaq", "won", "krw", "seoul"
        ]
        
        # ê°„ì ‘ ê´€ë ¨ í‚¤ì›Œë“œ
        indirect_keywords = [
            "semiconductor", "chip", "battery", "ev", "fed", "interest rate",
            "china", "japan", "trade", "tariff", "oil", "nvidia", "tsmc",
            "nasdaq", "s&p", "dow jones", "treasury"
        ]
        
        direct_count = sum(1 for kw in direct_keywords if kw in text)
        indirect_count = sum(1 for kw in indirect_keywords if kw in text)
        
        if direct_count >= 2:
            return {"is_relevant": True, "relevance_level": "high", "quick_reason": "ì§ì ‘ ì–¸ê¸‰"}
        elif direct_count >= 1:
            return {"is_relevant": True, "relevance_level": "medium", "quick_reason": "í•œêµ­ ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨"}
        elif indirect_count >= 3:
            return {"is_relevant": True, "relevance_level": "medium", "quick_reason": "ê°„ì ‘ ì˜í–¥ ê°€ëŠ¥ì„±"}
        elif indirect_count >= 1:
            return {"is_relevant": True, "relevance_level": "low", "quick_reason": "ì•½í•œ ì—°ê´€ì„±"}
        else:
            return {"is_relevant": False, "relevance_level": "none", "quick_reason": "ê´€ë ¨ì„± ì—†ìŒ"}
    
    def _rule_based_analysis(self, article: Dict) -> Dict:
        """ê·œì¹™ ê¸°ë°˜ ë¶„ì„ (í´ë°±)"""
        from collectors.finance_rss import detect_affected_sectors, calculate_korea_relevance
        
        title = article.get('title', '')
        description = article.get('description', '')
        
        sectors = detect_affected_sectors(title, description)
        relevance = calculate_korea_relevance(title, description)
        
        # ê°„ë‹¨í•œ ê°ì • ë¶„ì„
        text_lower = f"{title} {description}".lower()
        
        negative_words = ["drop", "fall", "decline", "crash", "risk", "concern", 
                          "warning", "cut", "ban", "restrict", "sanction", "loss"]
        positive_words = ["rise", "gain", "surge", "boost", "strong", "growth",
                          "profit", "deal", "invest", "expand", "record"]
        
        neg_count = sum(1 for w in negative_words if w in text_lower)
        pos_count = sum(1 for w in positive_words if w in text_lower)
        
        if neg_count > pos_count:
            direction = "negative"
            strategy = "ê´€ë§"
        elif pos_count > neg_count:
            direction = "positive"
            strategy = "ê´€ë§"
        else:
            direction = "neutral"
            strategy = "ê´€ë§"
        
        return {
            "korea_relevance": relevance,
            "impact_direction": direction,
            "confidence": 0.5,
            "affected_sectors": sectors,
            "impact_timing": "ì¥ì¤‘",
            "investment_strategy": strategy,
            "reasoning": "",
            "title_ko": "",
            "key_factors": [],
            "analysis_method": "rule_based"
        }


# ==============================================
# í¸ì˜ í•¨ìˆ˜ (ëª¨ë“ˆ ë ˆë²¨)
# ==============================================

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_analyzer_instance: Optional[GroqAnalyzer] = None

def _get_analyzer() -> GroqAnalyzer:
    """ì‹±ê¸€í†¤ ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _analyzer_instance
    if _analyzer_instance is None:
        _analyzer_instance = GroqAnalyzer()
    return _analyzer_instance


def analyze_with_groq(article: Dict, deep: bool = True) -> Dict:
    """
    ë‹¨ì¼ ê¸°ì‚¬ Groq ë¶„ì„
    
    Args:
        article: ë‰´ìŠ¤ ê¸°ì‚¬
        deep: Trueë©´ ì‹¬ì¸µë¶„ì„, Falseë©´ 1ì°¨ í•„í„°ë§ë§Œ
        
    Returns:
        Dict: ë¶„ì„ ê²°ê³¼
    """
    analyzer = _get_analyzer()
    
    if deep:
        return analyzer.analyze_deep(article)
    else:
        return analyzer.filter_korea_relevant(article)


def filter_korea_relevant_news(articles: List[Dict], 
                                rate_limit_delay: float = 2.0) -> Tuple[List[Dict], List[Dict]]:
    """
    1ì°¨ í•„í„°ë§: í•œêµ­ ê´€ë ¨ ë‰´ìŠ¤ë§Œ ì¶”ì¶œ (ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©)
    
    Args:
        articles: ì „ì²´ ë‰´ìŠ¤ ëª©ë¡
        rate_limit_delay: API í˜¸ì¶œ ê°„ ëŒ€ê¸° ì‹œê°„
        
    Returns:
        Tuple[relevant, not_relevant]: ê´€ë ¨ ë‰´ìŠ¤ì™€ ë¹„ê´€ë ¨ ë‰´ìŠ¤
    """
    logger.info(f"ğŸ” 1ì°¨ í•„í„°ë§ ì‹œì‘: {len(articles)}ê°œ ê¸°ì‚¬")
    
    analyzer = _get_analyzer()
    relevant = []
    not_relevant = []
    
    for i, article in enumerate(articles):
        try:
            result = analyzer.filter_korea_relevant(article)
            article["filter_result"] = result
            
            if result.get("is_relevant", False):
                relevant.append(article)
            else:
                not_relevant.append(article)
            
            if (i + 1) % 10 == 0:
                logger.info(f"   ì§„í–‰: {i + 1}/{len(articles)} (ê´€ë ¨: {len(relevant)}ê°œ)")
            
            time.sleep(rate_limit_delay)
            
        except Exception as e:
            logger.error(f"   âŒ í•„í„°ë§ ì‹¤íŒ¨: {e}")
            not_relevant.append(article)
    
    logger.info(f"   âœ… í•„í„°ë§ ì™„ë£Œ: {len(relevant)}ê°œ ê´€ë ¨, {len(not_relevant)}ê°œ ì œì™¸")
    return relevant, not_relevant


def analyze_news_batch_groq(articles: List[Dict], 
                            use_two_stage: bool = True,
                            rate_limit_delay: float = 3.0) -> List[Dict]:
    """
    ë°°ì¹˜ ë‰´ìŠ¤ ë¶„ì„ (2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸)
    
    Args:
        articles: ë¶„ì„í•  ê¸°ì‚¬ ëª©ë¡
        use_two_stage: Trueë©´ 2ë‹¨ê³„ ë¶„ì„, Falseë©´ ë°”ë¡œ ì‹¬ì¸µë¶„ì„
        rate_limit_delay: API í˜¸ì¶œ ê°„ ëŒ€ê¸° ì‹œê°„
        
    Returns:
        List[Dict]: ë¶„ì„ ê²°ê³¼ê°€ ì¶”ê°€ëœ ê¸°ì‚¬ ëª©ë¡
    """
    logger.info(f"ğŸ“Š Groq ë°°ì¹˜ ë¶„ì„ ì‹œì‘: {len(articles)}ê°œ ê¸°ì‚¬")
    
    analyzer = _get_analyzer()
    
    if not analyzer.available:
        logger.warning("âš ï¸ Groq API ì‚¬ìš© ë¶ˆê°€. ê·œì¹™ ê¸°ë°˜ ë¶„ì„ìœ¼ë¡œ í´ë°±")
        for article in articles:
            article["korea_impact"] = analyzer._rule_based_analysis(article)
        return articles
    
    # 2ë‹¨ê³„ ë¶„ì„
    if use_two_stage:
        # 1ë‹¨ê³„: ë¹ ë¥¸ í•„í„°ë§
        relevant, _ = filter_korea_relevant_news(articles, rate_limit_delay=2.0)
        
        logger.info(f"ğŸ”¬ 2ì°¨ ì‹¬ì¸µ ë¶„ì„: {len(relevant)}ê°œ ê¸°ì‚¬")
        
        # 2ë‹¨ê³„: ê´€ë ¨ ë‰´ìŠ¤ë§Œ ì‹¬ì¸µ ë¶„ì„
        for i, article in enumerate(relevant):
            try:
                impact = analyzer.analyze_deep(article)
                article["korea_impact"] = impact
                
                if (i + 1) % 5 == 0:
                    logger.info(f"   ì‹¬ì¸µë¶„ì„ ì§„í–‰: {i + 1}/{len(relevant)}")
                
                time.sleep(rate_limit_delay)
                
            except Exception as e:
                logger.error(f"   âŒ ì‹¬ì¸µë¶„ì„ ì‹¤íŒ¨: {e}")
                article["korea_impact"] = analyzer._rule_based_analysis(article)
        
        return relevant
    
    else:
        # ëª¨ë“  ê¸°ì‚¬ ì‹¬ì¸µë¶„ì„ (ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼)
        for i, article in enumerate(articles):
            try:
                impact = analyzer.analyze_deep(article)
                article["korea_impact"] = impact
                
                if (i + 1) % 5 == 0:
                    logger.info(f"   ì§„í–‰: {i + 1}/{len(articles)}")
                
                time.sleep(rate_limit_delay)
                
            except Exception as e:
                logger.error(f"   âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
                article["korea_impact"] = analyzer._rule_based_analysis(article)
        
        return articles


# ==============================================
# í…ŒìŠ¤íŠ¸
# ==============================================
if __name__ == "__main__":
    print("=" * 60)
    print("Groq ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # í…ŒìŠ¤íŠ¸ ê¸°ì‚¬
    test_articles = [
        {
            "title": "Fed signals potential rate cut amid cooling inflation",
            "description": "The Federal Reserve indicated it may reduce interest rates in the coming months as inflation shows signs of cooling, boosting global market sentiment.",
            "source": "Reuters"
        },
        {
            "title": "Samsung Electronics Q1 profit expected to surge on memory chip demand",
            "description": "Samsung Electronics is expected to report a significant profit increase in Q1 driven by strong memory chip demand from AI data centers.",
            "source": "WSJ"
        },
        {
            "title": "Local bakery wins best croissant award",
            "description": "A small bakery in Paris has been awarded the best croissant in the city for the third consecutive year.",
            "source": "Local News"
        }
    ]
    
    print("\n1ï¸âƒ£ 1ì°¨ í•„í„°ë§ í…ŒìŠ¤íŠ¸")
    print("-" * 40)
    
    analyzer = GroqAnalyzer()
    
    if analyzer.available:
        for article in test_articles:
            result = analyzer.filter_korea_relevant(article)
            print(f"\nì œëª©: {article['title'][:50]}...")
            print(f"ê´€ë ¨ì„±: {result.get('relevance_level')} - {result.get('quick_reason')}")
        
        print("\n2ï¸âƒ£ 2ì°¨ ì‹¬ì¸µë¶„ì„ í…ŒìŠ¤íŠ¸ (ì²« ë²ˆì§¸ ê¸°ì‚¬)")
        print("-" * 40)
        
        deep_result = analyzer.analyze_deep(test_articles[1])
        print(json.dumps(deep_result, ensure_ascii=False, indent=2))
    else:
        print("âš ï¸ Groq API ì‚¬ìš© ë¶ˆê°€. GROQ_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        print("   ê·œì¹™ ê¸°ë°˜ ë¶„ì„ ê²°ê³¼:")
        for article in test_articles:
            result = analyzer._rule_based_relevance(article)
            print(f"\nì œëª©: {article['title'][:50]}...")
            print(f"ê´€ë ¨ì„±: {result.get('relevance_level')}")
