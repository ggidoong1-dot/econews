"""
ê¸€ë¡œë²Œ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° (v3.1)
ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë ˆì´ì–´: ì—¬ëŸ¬ ìˆ˜ì§‘ê¸° ëª¨ë“ˆì„ í˜¸ì¶œí•˜ê³  ê²°ê³¼ë¥¼ ì •ì œí•˜ì—¬ DBì— ì €ì¥í•©ë‹ˆë‹¤.
í•œêµ­ ì£¼ì‹ì‹œì¥ ì˜í–¥ ë¶„ì„ì„ ìœ„í•œ ê²½ì œ ë‰´ìŠ¤ ìˆ˜ì§‘ ê¸°ëŠ¥ ì¶”ê°€.
"""
import time
from typing import List, Dict
import config
import database as db

# ëª¨ë“ˆí™”ëœ ìˆ˜ì§‘ê¸°/ìœ í‹¸ ì„í¬íŠ¸
import collector_utils as utils
from collectors.rss import fetch_google_news, fetch_reddit_rss, fetch_direct_rss, fetch_newsapi
from collectors.scraper import fetch_naver_news
from collectors.finance_rss import fetch_finance_rss, fetch_finance_rss_all

# ë¡œê±° ì„¤ì •
logger = config.setup_logger(__name__)


# ==============================================
# ë©”ì¸ ìˆ˜ì§‘ ë¡œì§
# ==============================================

def run_collector():
    """ë©”ì¸ ìˆ˜ì§‘ê¸° ì‹¤í–‰ - í†µê³„ ì •ë³´ ë°˜í™˜"""
    logger.info("=" * 60)
    logger.info("ğŸš€ ê¸€ë¡œë²Œ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì‹œì‘ (v3.0)")
    logger.info("=" * 60)
    
    # í†µê³„ ë³€ìˆ˜
    stats = {
        "total_crawled": 0,      # í¬ë¡¤ë§í•œ ì´ ê°œìˆ˜
        "duplicates_removed": 0,  # ì¤‘ë³µ ì œê±°ëœ ê°œìˆ˜
        "invalid_removed": 0,     # ê²€ì¦ ì‹¤íŒ¨í•œ ê°œìˆ˜
        "total_valid": 0,         # ìœ íš¨í•œ ê¸°ì‚¬ ê°œìˆ˜
        "insert_success": 0,      # INSERT ì„±ê³µ
        "insert_failed": 0,       # INSERT ì‹¤íŒ¨
        "insert_skipped": 0       # INSERT ë¬´ì‹œ (ê²€ì¦ ì‹¤íŒ¨ ë“±)
    }
    
    # 1. ì¤‘ë³µ ì²´í¬ìš© ê¸°ì¡´ ë§í¬ ë¡œë“œ
    existing_links = db.get_recent_links(config.COLLECTOR_LOOKBACK_DAYS)
    logger.info(f"ğŸ“Š ê¸°ì¡´ ë§í¬: {len(existing_links)}ê°œ (ìµœê·¼ {config.COLLECTOR_LOOKBACK_DAYS}ì¼)")
    
    # 2. ê¸ˆì§€ì–´ ë¡œë“œ
    ban_words = db.get_ban_words()
    logger.info(f"ğŸš« ê¸ˆì§€ì–´: {len(ban_words)}ê°œ")
    
    # 3. í‚¤ì›Œë“œ ë¡œë“œ (Supabaseì—ì„œ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°)
    all_keywords = db.get_keywords()
    
    # í•œê¸€/ì˜ë¬¸ ìë™ ë¶„ë¥˜
    keywords_ko = [k for k in all_keywords if any('\uac00' <= c <= '\ud7a3' for c in k)]
    keywords_en = [k for k in all_keywords if k not in keywords_ko]
    
    # ê¸°ë³¸ê°’ í´ë°± (DB í‚¤ì›Œë“œê°€ ì—†ì„ ê²½ìš°)
    if not keywords_en:
        keywords_en = config.KEYWORDS_EN
    if not keywords_ko:
        keywords_ko = config.KEYWORDS_KO
    
    logger.info(f"ğŸ”‘ ê²€ìƒ‰ í‚¤ì›Œë“œ: ì˜ë¬¸ {len(keywords_en)}ê°œ, í•œê¸€ {len(keywords_ko)}ê°œ (ì´ {len(all_keywords)}ê°œ)")
    
    # 4. ê° ì†ŒìŠ¤ë³„ ìˆ˜ì§‘
    all_articles = []
    
    if config.NEWS_SOURCES['google']['enabled']:
        all_articles.extend(fetch_google_news(keywords_en))
    
    # Bing / NewsAPI may be optional implementations; call only if functions exist
    if config.NEWS_SOURCES.get('bing', {}).get('enabled'):
        if 'fetch_bing_news' in globals():
            all_articles.extend(fetch_bing_news(keywords_en))
        else:
            logger.warning("   âš ï¸ Bing ë‰´ìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜ê°€ êµ¬í˜„ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒëµí•©ë‹ˆë‹¤.")

    if config.NEWS_SOURCES.get('newsapi', {}).get('enabled'):
        all_articles.extend(fetch_newsapi(keywords_en))
    
    if config.NEWS_SOURCES['naver']['enabled']:
        all_articles.extend(fetch_naver_news(keywords_ko))
    
    # Reddit RSS
    if config.REDDIT_SOURCES:
        all_articles.extend(fetch_reddit_rss(config.REDDIT_SOURCES))
    
    # ì§ì ‘ RSS
    if config.DIRECT_RSS_SOURCES:
        all_articles.extend(fetch_direct_rss(config.DIRECT_RSS_SOURCES, keywords_en))
    
    # ê²½ì œ/ê¸ˆìœµ ì „ë¬¸ ë‰´ìŠ¤ RSS (í•œêµ­ ì‹œì¥ ì˜í–¥ ë¶„ì„ìš©)
    if hasattr(config, 'FINANCE_RSS_SOURCES') and config.FINANCE_RSS_SOURCES:
        logger.info("ğŸ“Š ê²½ì œ/ê¸ˆìœµ ì „ë¬¸ ë‰´ìŠ¤ ìˆ˜ì§‘...")
        all_articles.extend(fetch_finance_rss_all())  # ì „ì²´ ìˆ˜ì§‘ í›„ AIê°€ í•„í„°ë§
    
    stats["total_crawled"] = len(all_articles)
    logger.info(f"\nğŸ“¥ ì´ í¬ë¡¤ë§: {stats['total_crawled']}ê°œ")
    
    # 5. í•„í„°ë§ ë° ì •ì œ
    logger.info("ğŸ” í•„í„°ë§ ì‹œì‘...")
    valid_articles = []
    
    for article in all_articles:
        # ì¤‘ë³µ ì²´í¬
        if article['link'] in existing_links:
            stats["duplicates_removed"] += 1
            continue
        
        # ìœ íš¨ì„± ê²€ì‚¬
        if not utils.is_valid_article(article, ban_words):
            stats["invalid_removed"] += 1
            continue
        
        # ì¶”ê°€ í•„ë“œ ì„¤ì •
        article['content_hash'] = utils.generate_content_hash(article['link'])
        article['is_processed'] = False
        
        valid_articles.append(article)
        existing_links.add(article['link'])  # ì´ë²ˆ ì‹¤í–‰ ë‚´ ì¤‘ë³µ ë°©ì§€
    
    stats["total_valid"] = len(valid_articles)
    stats["insert_skipped"] = stats["duplicates_removed"] + stats["invalid_removed"]
    
    logger.info(f"âœ… ìœ íš¨ ê¸°ì‚¬: {stats['total_valid']}ê°œ")
    logger.info(f"   - ì¤‘ë³µ ì œê±°: {stats['duplicates_removed']}ê°œ")
    logger.info(f"   - ê²€ì¦ ì‹¤íŒ¨: {stats['invalid_removed']}ê°œ")
    
    # 6. DB ì €ì¥
    if valid_articles:
        saved_count = db.save_news_batch(valid_articles)
        stats["insert_success"] = saved_count
        stats["insert_failed"] = stats["total_valid"] - saved_count
        logger.info(f"ğŸ’¾ DB ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ ì„±ê³µ, {stats['insert_failed']}ê°œ ì‹¤íŒ¨")
    else:
        logger.info("ğŸ“­ ì €ì¥í•  ìƒˆ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        stats["insert_failed"] = 0
    
    # 7. ë§ˆì§€ë§‰ ì‹¤í–‰ ì‹œê°„ ì—…ë°ì´íŠ¸
    db.update_last_run()
    
    # 8. ìµœì¢… í†µê³„ ë¡œê¹…
    logger.info("\n" + "=" * 70)
    logger.info("ğŸ“Š ìˆ˜ì§‘ ì‘ì—… ìµœì¢… í†µê³„")
    logger.info("=" * 70)
    logger.info(f"ğŸŒ í¬ë¡¤ë§í•œ ê¸°ì‚¬:        {stats['total_crawled']:6d}ê°œ")
    logger.info(f"âœ… ìœ íš¨í•œ ê¸°ì‚¬:         {stats['total_valid']:6d}ê°œ")
    logger.info(f"   â”œâ”€ INSERT ì„±ê³µ:     {stats['insert_success']:6d}ê°œ")
    logger.info(f"   â””â”€ INSERT ì‹¤íŒ¨:     {stats['insert_failed']:6d}ê°œ")
    logger.info(f"âŒ ë¬´ì‹œëœ ê¸°ì‚¬:         {stats['insert_skipped']:6d}ê°œ")
    logger.info(f"   â”œâ”€ ì¤‘ë³µ ê¸°ì‚¬:       {stats['duplicates_removed']:6d}ê°œ")
    logger.info(f"   â””â”€ ê²€ì¦ ì‹¤íŒ¨:       {stats['invalid_removed']:6d}ê°œ")
    success_rate = (stats['insert_success'] / stats['total_crawled'] * 100) if stats['total_crawled'] > 0 else 0
    logger.info(f"ğŸ“ˆ ì„±ê³µë¥ :              {success_rate:6.1f}%")
    logger.info("=" * 70)
    
    # ì‹¤íŒ¨í•œ ê²½ìš° ê²½ê³ 
    if stats['insert_failed'] > 0:
        logger.error(f"\nğŸš¨ ì£¼ì˜: {stats['insert_failed']}ê°œ ê¸°ì‚¬ INSERT ì‹¤íŒ¨!")
        logger.error(f"   failed_articles.jsonl íŒŒì¼ í™•ì¸: logs/failed_articles.jsonl")
        logger.error(f"   ë¶„ì„ ëª…ë ¹ì–´: python log_analyzer.py analyze\n")
        import traceback
        traceback.print_exc()
    # ê²°ê³¼ ë°˜í™˜
    return stats
