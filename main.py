#!/usr/bin/env python3
"""
ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ìˆ˜ì§‘ â†’ ë¶„ì„ â†’ ë¦¬í¬íŠ¸ ìƒì„±ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
"""
import os
import sys
# ğŸ‘‡ [ì¤‘ìš”] .env íŒŒì¼ì„ ì½ì–´ì˜¤ëŠ” ì½”ë“œê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!
from dotenv import load_dotenv
load_dotenv()

import argparse
import time
from datetime import datetime, timezone
import config
import database as db
import collector
import analyzer
import reporter

# ë¡œê±° ì„¤ì •
logger = config.setup_logger(__name__)

# ==============================================
# ì „ì²´ íŒŒì´í”„ë¼ì¸
# ==============================================

def run_full_pipeline(force: bool = False):
    """
    ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰: ìˆ˜ì§‘ â†’ ë¶„ì„ â†’ ë¦¬í¬íŠ¸
    
    Args:
        force: ìˆ˜ì§‘ ì£¼ê¸° ë¬´ì‹œí•˜ê³  ê°•ì œ ì‹¤í–‰
    """
    logger.info("\n" + "=" * 70)
    logger.info("ğŸš€ Well-Dying Archive ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    logger.info(f"   ì‹¤í–‰ ì‹œê°: {datetime.now(timezone.utc).isoformat()}")
    logger.info("=" * 70)
    
    start_time = time.time()
    
    # í†µê³„ ìˆ˜ì§‘
    pipeline_stats = {
        "collector": None,
        "analyzer": None,
        "reporter": None
    }
    
    try:
        # 1. ìˆ˜ì§‘ ì£¼ê¸° ì²´í¬
        if not force:
            should_run = db.should_run_collector()
            if not should_run:
                logger.info("â­ï¸  ìˆ˜ì§‘ ì£¼ê¸°ê°€ ì•„ë‹ˆë¯€ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                logger.info("   ê°•ì œ ì‹¤í–‰í•˜ë ¤ë©´: python main.py --force")
                return
        else:
            logger.info("âš¡ ê°•ì œ ì‹¤í–‰ ëª¨ë“œ")
        
        # 2. ë‰´ìŠ¤ ìˆ˜ì§‘
        logger.info("\n" + "â”€" * 70)
        logger.info("ğŸ“¡ STEP 1: ë‰´ìŠ¤ ìˆ˜ì§‘")
        logger.info("â”€" * 70)
        pipeline_stats["collector"] = collector.run_collector()
        
        # 3. AI ë¶„ì„ (30ì´ˆ ëŒ€ê¸° í›„)
        logger.info("\n" + "â”€" * 70)
        logger.info("ğŸ¤– STEP 2: AI ë¶„ì„ (30ì´ˆ í›„ ì‹œì‘)")
        logger.info("â”€" * 70)
        time.sleep(30)
        analyzer.run_analyzer(batch_size=20)
        
        # 4. ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„± (ì„ íƒì‚¬í•­)
        logger.info("\n" + "â”€" * 70)
        logger.info("ğŸ“Š STEP 3: ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„± (ìŠ¤í‚µ ê°€ëŠ¥)")
        logger.info("â”€" * 70)
        
        # í•˜ë£¨ì— í•œ ë²ˆë§Œ ë¦¬í¬íŠ¸ ìƒì„± (ì˜ˆ: ì˜¤ì „ 9ì‹œ)
        current_hour = datetime.now(timezone.utc).hour
        if current_hour == 1 or force:  # UTC 1ì‹œ = í•œêµ­ 10ì‹œ
            reporter.run_reporter(hours=24)
        else:
            logger.info(f"   í˜„ì¬ ì‹œê°: UTC {current_hour}ì‹œ")
            logger.info("   ë¦¬í¬íŠ¸ëŠ” UTC 1ì‹œ(í•œêµ­ 10ì‹œ)ì—ë§Œ ìƒì„±ë©ë‹ˆë‹¤.")
            logger.info("   ê°•ì œ ìƒì„±í•˜ë ¤ë©´: python main.py report")
        
        # 5. ìµœì¢… í†µê³„ ì¶œë ¥
        elapsed = time.time() - start_time
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ¯ ì „ì²´ íŒŒì´í”„ë¼ì¸ ìµœì¢… í†µê³„")
        logger.info("=" * 70)
        
        if pipeline_stats["collector"]:
            collector_stats = pipeline_stats["collector"]
            logger.info("\nğŸ“¡ [ìˆ˜ì§‘ ë‹¨ê³„]")
            logger.info(f"   ğŸŒ í¬ë¡¤ë§í•œ ê¸°ì‚¬:    {collector_stats.get('total_crawled', 0):6d}ê°œ")
            logger.info(f"   âœ… INSERT ì„±ê³µ:      {collector_stats.get('insert_success', 0):6d}ê°œ")
            logger.info(f"   âŒ INSERT ì‹¤íŒ¨:      {collector_stats.get('insert_failed', 0):6d}ê°œ")
            logger.info(f"   â­ï¸  ë¬´ì‹œëœ ê¸°ì‚¬:      {collector_stats.get('insert_skipped', 0):6d}ê°œ")
            logger.info(f"      â”œâ”€ ì¤‘ë³µ:          {collector_stats.get('duplicates_removed', 0):6d}ê°œ")
            logger.info(f"      â””â”€ ê²€ì¦ ì‹¤íŒ¨:     {collector_stats.get('invalid_removed', 0):6d}ê°œ")
            
            total = collector_stats.get('total_crawled', 1)
            success_rate = (collector_stats.get('insert_success', 0) / total * 100)
            logger.info(f"   ğŸ“ˆ ì„±ê³µë¥ :           {success_rate:6.1f}%")
        
        logger.info("\n" + "=" * 70)
        logger.info(f"âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ)")
        logger.info("=" * 70)
        
    except KeyboardInterrupt:
        logger.info("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        sys.exit(1)
    except Exception as e:
        logger.error(f"\nâŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

# ==============================================
# ê°œë³„ ëª¨ë“ˆ ì‹¤í–‰
# ==============================================

def run_collector_only():
    """ìˆ˜ì§‘ë§Œ ì‹¤í–‰"""
    logger.info("ğŸ“¡ ìˆ˜ì§‘ê¸°ë§Œ ì‹¤í–‰")
    stats = collector.run_collector()

    # ì•ˆì „ì„±: collectorê°€ ì˜ˆì™¸ë¡œ Noneì„ ë°˜í™˜í•˜ë”ë¼ë„ ì²˜ë¦¬
    if not stats:
        logger.error("âŒ ìˆ˜ì§‘ ì‘ì—… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    # í†µê³„ ì¶œë ¥
    logger.info("\n" + "=" * 70)
    logger.info("ğŸ“Š ìˆ˜ì§‘ ì‘ì—… ìš”ì•½")
    logger.info("=" * 70)
    logger.info(f"ğŸŒ í¬ë¡¤ë§í•œ ê¸°ì‚¬:      {stats.get('total_crawled', 0):6d}ê°œ")
    logger.info(f"âœ… INSERT ì„±ê³µ:        {stats.get('insert_success', 0):6d}ê°œ")
    logger.info(f"âŒ INSERT ì‹¤íŒ¨:        {stats.get('insert_failed', 0):6d}ê°œ")
    logger.info(f"â­ï¸  ë¬´ì‹œëœ ê¸°ì‚¬:        {stats.get('insert_skipped', 0):6d}ê°œ")
    logger.info(f"   â”œâ”€ ì¤‘ë³µ:            {stats.get('duplicates_removed', 0):6d}ê°œ")
    logger.info(f"   â””â”€ ê²€ì¦ ì‹¤íŒ¨:       {stats.get('invalid_removed', 0):6d}ê°œ")
    logger.info("=" * 70)


def run_analyzer_only(batch_size: int = 10):
    """ë¶„ì„ë§Œ ì‹¤í–‰"""
    logger.info(f"ğŸ¤– ë¶„ì„ê¸°ë§Œ ì‹¤í–‰ (ë°°ì¹˜ í¬ê¸°: {batch_size})")
    analyzer.run_analyzer(batch_size=batch_size)


def run_reporter_only(hours: int = 24):
    """ë¦¬í¬íŠ¸ë§Œ ìƒì„±"""
    logger.info(f"ğŸ“Š ë¦¬í¬í„°ë§Œ ì‹¤í–‰ (ìµœê·¼ {hours}ì‹œê°„)")
    reporter.run_reporter(hours=hours)


def show_statistics():
    """í†µê³„ ì¶œë ¥"""
    logger.info("\n" + "=" * 70)
    logger.info("ğŸ“Š ì‹œìŠ¤í…œ í†µê³„")
    logger.info("=" * 70)
    
    # ìµœê·¼ 7ì¼ í†µê³„
    try:
        stats = db.get_statistics(days=7)
        
        print(f"\n[ìµœê·¼ 7ì¼ í†µê³„]")
        print(f"  ì´ ê¸°ì‚¬: {stats['total_articles']}ê°œ")
        print(f"  ì²˜ë¦¬ ì™„ë£Œ: {stats['processed_articles']}ê°œ")
        print(f"  ë¯¸ì²˜ë¦¬: {stats['unprocessed_articles']}ê°œ")
        print(f"  ì²˜ë¦¬ìœ¨: {stats['processing_rate']}%")
        
        # ì†ŒìŠ¤ë³„ í†µê³„
        if stats.get('sources'):
            print(f"\n[ì†ŒìŠ¤ë³„ ê¸°ì‚¬ ìˆ˜]")
            source_counts = {}
            for item in stats['sources']:
                source = item.get('source', 'Unknown')
                source_counts[source] = source_counts.get(source, 0) + 1
            
            for source, count in sorted(source_counts.items(), key=lambda x: x[1], reverse=True):
                print(f"  {source}: {count}ê°œ")
        
        # ì„¤ì • ì •ë³´
        settings = db.get_settings()
        print(f"\n[ìˆ˜ì§‘ ì„¤ì •]")
        print(f"  ìˆ˜ì§‘ ì£¼ê¸°: {settings.get('interval_minutes')}ë¶„")
        print(f"  ë§ˆì§€ë§‰ ì‹¤í–‰: {settings.get('last_run')}")
        
        should_run = db.should_run_collector()
        print(f"  ë‹¤ìŒ ì‹¤í–‰: {'ê°€ëŠ¥' if should_run else 'ëŒ€ê¸° ì¤‘'}")

    except Exception as e:
        logger.error(f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
    
    logger.info("=" * 70)


def test_connection():
    """DB ë° API ì—°ê²° í…ŒìŠ¤íŠ¸"""
    logger.info("\n" + "=" * 70)
    logger.info("ğŸ”Œ ì—°ê²° í…ŒìŠ¤íŠ¸")
    logger.info("=" * 70)
    
    # 1. Supabase ì—°ê²°
    print("\n[Supabase]")
    try:
        db.ensure_connection()
        print("  âœ… ì—°ê²° ì„±ê³µ")
        
        countries = db.get_countries()
        keywords = db.get_keywords()
        print(f"  - êµ­ê°€: {len(countries)}ê°œ")
        print(f"  - í‚¤ì›Œë“œ: {len(keywords)}ê°œ")
    except Exception as e:
        print(f"  âŒ ì—°ê²° ì‹¤íŒ¨: {e}")
    
    # 2. Gemini API
    print("\n[Gemini API]")
    try:
        api_url = config.get_gemini_api_url()
        print(f"  âœ… API URL ìƒì„± ì„±ê³µ")
        print(f"  - ëª¨ë¸: {config.GEMINI_MODEL}")
        print(f"  - URL: {api_url[:80]}...")
    except Exception as e:
        print(f"  âŒ API ì„¤ì • ì˜¤ë¥˜: {e}")
    
    # 3. Telegram
    print("\n[Telegram]")
    if config.TELEGRAM_BOT_TOKEN and config.TELEGRAM_CHAT_ID:
        print("  âœ… ì¸ì¦ ì •ë³´ ì„¤ì •ë¨")
        print(f"  - ì±„íŒ… ID: {config.TELEGRAM_CHAT_ID}")
    else:
        print("  âš ï¸ ì¸ì¦ ì •ë³´ ì—†ìŒ (ì„ íƒì‚¬í•­)")
    
    # 4. NewsAPI
    print("\n[NewsAPI]")
    if config.NEWSAPI_KEY:
        print("  âœ… API í‚¤ ì„¤ì •ë¨")
    else:
        print("  âš ï¸ API í‚¤ ì—†ìŒ (ì„ íƒì‚¬í•­)")
        print("  ë¬´ë£Œ í‚¤ ë°œê¸‰: https://newsapi.org/register")
    
    logger.info("\n" + "=" * 70)

# ==============================================
# CLI ì¸í„°í˜ì´ìŠ¤
# ==============================================

def main():
    parser = argparse.ArgumentParser(
        description='Well-Dying Archive - ê¸€ë¡œë²Œ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë¶„ì„ ì‹œìŠ¤í…œ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
  python main.py                    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
  python main.py --force            # ì£¼ê¸° ë¬´ì‹œí•˜ê³  ê°•ì œ ì‹¤í–‰
  python main.py collect            # ìˆ˜ì§‘ë§Œ ì‹¤í–‰
  python main.py analyze            # ë¶„ì„ë§Œ ì‹¤í–‰
  python main.py report             # ë¦¬í¬íŠ¸ë§Œ ìƒì„±
  python main.py stats              # í†µê³„ í™•ì¸
  python main.py test               # ì—°ê²° í…ŒìŠ¤íŠ¸
        """
    )
    
    parser.add_argument(
        'command',
        nargs='?',
        choices=['collect', 'analyze', 'report', 'stats', 'test'],
        help='ì‹¤í–‰í•  ëª…ë ¹'
    )
    
    parser.add_argument(
        '--force',
        action='store_true',
        help='ìˆ˜ì§‘ ì£¼ê¸° ë¬´ì‹œí•˜ê³  ê°•ì œ ì‹¤í–‰'
    )
    
    parser.add_argument(
        '--batch-size',
        type=int,
        default=10,
        help='ë¶„ì„ ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 10)'
    )
    
    parser.add_argument(
        '--hours',
        type=int,
        default=24,
        help='ë¦¬í¬íŠ¸ ì‹œê°„ ë²”ìœ„ (ê¸°ë³¸: 24)'
    )
    
    args = parser.parse_args()
    
    # ëª…ë ¹ ì‹¤í–‰
    if args.command == 'collect':
        run_collector_only()
    elif args.command == 'analyze':
        run_analyzer_only(batch_size=args.batch_size)
    elif args.command == 'report':
        run_reporter_only(hours=args.hours)
    elif args.command == 'stats':
        show_statistics()
    elif args.command == 'test':
        test_connection()
    else:
        # ê¸°ë³¸: ì „ì²´ íŒŒì´í”„ë¼ì¸
        run_full_pipeline(force=args.force)


if __name__ == "__main__":
    main()