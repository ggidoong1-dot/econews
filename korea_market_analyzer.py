"""
í•œêµ­ ì‹œì¥ ì˜í–¥ ë¶„ì„ê¸°
ê¸€ë¡œë²Œ ë‰´ìŠ¤ê°€ í•œêµ­ ì£¼ì‹ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ AIë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
"""
import json
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timezone
import config

logger = config.setup_logger(__name__)

# Gemini API - API í‚¤ê°€ ìˆì„ ë•Œë§Œ ì´ˆê¸°í™”
GEMINI_AVAILABLE = False
if config.GOOGLE_API_KEY:
    try:
        import google.generativeai as genai
        genai.configure(api_key=config.GOOGLE_API_KEY)
        GEMINI_AVAILABLE = True
        logger.info("âœ… Gemini API ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"Gemini API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
else:
    logger.info("â„¹ï¸ GOOGLE_API_KEY ë¯¸ì„¤ì • - Gemini ì‚¬ìš© ë¶ˆê°€")

# Groq API - API í‚¤ê°€ ìˆì„ ë•Œë§Œ ì´ˆê¸°í™”
GROQ_AVAILABLE = False
if config.GROQ_API_KEY:
    try:
        from groq import Groq
        groq_client = Groq(api_key=config.GROQ_API_KEY)
        GROQ_AVAILABLE = True
        logger.info("âœ… Groq API ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"Groq API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
else:
    logger.info("â„¹ï¸ GROQ_API_KEY ë¯¸ì„¤ì • - Groq ì‚¬ìš© ë¶ˆê°€")


# ==============================================
# ì„¹í„°-ì¢…ëª© ë§¤í•‘ ë°ì´í„°
# ==============================================
SECTOR_STOCKS = {
    "ë°˜ë„ì²´": [
        {"code": "005930", "name": "ì‚¼ì„±ì „ì", "weight": "high"},
        {"code": "000660", "name": "SKí•˜ì´ë‹‰ìŠ¤", "weight": "high"},
        {"code": "005935", "name": "ì‚¼ì„±ì „ììš°", "weight": "medium"},
    ],
    "2ì°¨ì „ì§€": [
        {"code": "373220", "name": "LGì—ë„ˆì§€ì†”ë£¨ì…˜", "weight": "high"},
        {"code": "006400", "name": "ì‚¼ì„±SDI", "weight": "high"},
        {"code": "247540", "name": "ì—ì½”í”„ë¡œë¹„ì— ", "weight": "medium"},
        {"code": "086520", "name": "ì—ì½”í”„ë¡œ", "weight": "medium"},
    ],
    "ìë™ì°¨": [
        {"code": "005380", "name": "í˜„ëŒ€ì°¨", "weight": "high"},
        {"code": "000270", "name": "ê¸°ì•„", "weight": "high"},
        {"code": "012330", "name": "í˜„ëŒ€ëª¨ë¹„ìŠ¤", "weight": "medium"},
    ],
    "ë°”ì´ì˜¤": [
        {"code": "207940", "name": "ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤", "weight": "high"},
        {"code": "068270", "name": "ì…€íŠ¸ë¦¬ì˜¨", "weight": "high"},
        {"code": "326030", "name": "SKë°”ì´ì˜¤íŒœ", "weight": "medium"},
    ],
    "IT/ì¸í„°ë„·": [
        {"code": "035420", "name": "NAVER", "weight": "high"},
        {"code": "035720", "name": "ì¹´ì¹´ì˜¤", "weight": "high"},
        {"code": "263750", "name": "í„ì–´ë¹„ìŠ¤", "weight": "low"},
    ],
    "ê¸ˆìœµ": [
        {"code": "105560", "name": "KBê¸ˆìœµ", "weight": "high"},
        {"code": "055550", "name": "ì‹ í•œì§€ì£¼", "weight": "high"},
        {"code": "316140", "name": "ìš°ë¦¬ê¸ˆìœµì§€ì£¼", "weight": "medium"},
    ],
    "ì¡°ì„ ": [
        {"code": "329180", "name": "HDí˜„ëŒ€ì¤‘ê³µì—…", "weight": "high"},
        {"code": "009540", "name": "HDí•œêµ­ì¡°ì„ í•´ì–‘", "weight": "high"},
        {"code": "010620", "name": "HDí˜„ëŒ€ë¯¸í¬", "weight": "medium"},
    ],
    "í™”í•™/ì •ìœ ": [
        {"code": "051910", "name": "LGí™”í•™", "weight": "high"},
        {"code": "096770", "name": "SKì´ë…¸ë² ì´ì…˜", "weight": "high"},
        {"code": "011170", "name": "ë¡¯ë°ì¼€ë¯¸ì¹¼", "weight": "medium"},
    ],
    "ì² ê°•": [
        {"code": "005490", "name": "POSCOí™€ë”©ìŠ¤", "weight": "high"},
        {"code": "004020", "name": "í˜„ëŒ€ì œì² ", "weight": "medium"},
    ],
    "ë°©ì‚°": [
        {"code": "012450", "name": "í•œí™”ì—ì–´ë¡œìŠ¤í˜ì´ìŠ¤", "weight": "high"},
        {"code": "079550", "name": "LIGë„¥ìŠ¤ì›", "weight": "high"},
        {"code": "047810", "name": "í•œêµ­í•­ê³µìš°ì£¼", "weight": "medium"},
    ],
    "ì›ìë ¥": [
        {"code": "034020", "name": "ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹°", "weight": "high"},
        {"code": "052690", "name": "í•œì „ê¸°ìˆ ", "weight": "high"},
    ],
    "ì—”í„°": [
        {"code": "352820", "name": "HYBE", "weight": "high"},
        {"code": "035900", "name": "JYP Ent.", "weight": "medium"},
        {"code": "041510", "name": "SM", "weight": "medium"},
    ],
}


# ==============================================
# AI ê¸°ë°˜ í•œêµ­ ì‹œì¥ ì˜í–¥ ë¶„ì„
# ==============================================


def analyze_korea_impact_batch(articles: List[Dict], mode: str = "auto") -> List[Dict]:
    """
    ë‰´ìŠ¤ ë°°ì¹˜ë¥¼ í•œ ë²ˆì— ë¶„ì„í•©ë‹ˆë‹¤ (Gemini ë˜ëŠ” Groq ì‚¬ìš©).
    
    Args:
        articles: ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        mode: ë¶„ì„ ëª¨ë“œ ("auto", "gemini", "groq")
        
    Returns:
        List[Dict]: ë¶„ì„ ê²°ê³¼ê°€ í¬í•¨ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
    """
    if not articles:
        return []

    # ì‚¬ìš©í•  API ê²°ì • logic
    use_groq = False
    
    if mode == "groq":
        if GROQ_AVAILABLE:
            use_groq = True
        else:
            logger.warning("Groq ëª¨ë“œ ì„ íƒë˜ì—ˆìœ¼ë‚˜ ì‚¬ìš© ë¶ˆê°€. Geminië¡œ í´ë°±.")
            if not GEMINI_AVAILABLE:
                logger.warning("Geminië„ ì‚¬ìš© ë¶ˆê°€. ê·œì¹™ ê¸°ë°˜ ë¶„ì„.")
                return _fallback_batch(articles)
                
    elif mode == "gemini":
        if not GEMINI_AVAILABLE:
            if GROQ_AVAILABLE:
                logger.warning("Gemini ì‚¬ìš© ë¶ˆê°€. Groqë¡œ í´ë°±.")
                use_groq = True
            else:
                return _fallback_batch(articles)
                
    else: # auto
        # ê¸°ë³¸ì ìœ¼ë¡œ Groq ì„ í˜¸ (ë” ë¹ ë¥´ê³  ì•ˆì •ì ì¼ ê²½ìš°)
        if GROQ_AVAILABLE:
            use_groq = True
        elif GEMINI_AVAILABLE:
            use_groq = False
        else:
            return _fallback_batch(articles)

    # 1. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt_intro = """ë‹¹ì‹ ì€ í•œêµ­ ì£¼ì‹ì‹œì¥ ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
ì•„ë˜ ì œê³µëœ ê¸€ë¡œë²Œ ë‰´ìŠ¤ë“¤ì´ í•œêµ­ ì£¼ì‹ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

[ë¶„ì„ ìš”ì²­]
ê° ë‰´ìŠ¤ì— ëŒ€í•´ ì•„ë˜ JSON í˜•ì‹ì˜ ê°ì²´ë¥¼ ìƒì„±í•˜ê³ , ì´ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ì–´ì„œ ë°˜í™˜í•´ì£¼ì„¸ìš”.
ì‘ë‹µì€ ì˜¤ì§ JSON ë¦¬ìŠ¤íŠ¸ë§Œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

ê°œë³„ ë‰´ìŠ¤ ë¶„ì„ í¬ë§· (JSON):
{
    "id": "ë‰´ìŠ¤ID",
    "korea_relevance": "high/medium/low/none",
    "impact_direction": "positive/negative/neutral",
    "confidence": 0.0-1.0,
    "affected_sectors": ["ì„¹í„°ëª…1", "ì„¹í„°ëª…2"],
    "reasoning": "í•œêµ­ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ì„¤ëª… (í•œê¸€, 100ì ì´ë‚´)",
    "title_ko": "í•œê¸€ ì œëª© ë²ˆì—­"
}

[ì£¼ì˜ì‚¬í•­]
- affected_sectorsëŠ” ë‹¤ìŒ ì¤‘ì—ì„œë§Œ ì„ íƒ: ë°˜ë„ì²´, 2ì°¨ì „ì§€, ìë™ì°¨, ë°”ì´ì˜¤, IT/ì¸í„°ë„·, ê¸ˆìœµ, ì¡°ì„ , í™”í•™/ì •ìœ , ì² ê°•, ë°©ì‚°, ì›ìë ¥, ì—”í„°
- korea_relevanceê°€ "none"ì´ë©´ reasoningì€ ê°„ë‹¨íˆ ì ê³  ë‚˜ë¨¸ì§€ í•„ë“œëŠ” ê¸°ë³¸ê°’.
- ìˆœì„œë¥¼ ë°˜ë“œì‹œ ì§€ì¼œì£¼ì„¸ìš”.

[ë‰´ìŠ¤ ëª©ë¡]
"""
    
    news_items = []
    for idx, article in enumerate(articles):
        # ì„ì‹œ ID ë¶€ì—¬ (ìˆœì„œ ì¶”ì ìš©)
        article_id = f"news_{idx}"
        title = article.get('title', '')
        description = article.get('description', '')[:300]
        source = article.get('source', '')
        
        news_items.append(f"""
ID: {article_id}
ì œëª©: {title}
ë‚´ìš©: {description}
ì¶œì²˜: {source}
---""")
        
    full_prompt = prompt_intro + "\n".join(news_items)

    try:
        text = ""
        
        if use_groq:
            # Groq í˜¸ì¶œ
            # ìƒˆë²½ ì‹œê°„ëŒ€(ì‹¬ì¸µ ë¶„ì„) vs í‰ì‹œ(ë¹ ë¥¸ ë¶„ì„) êµ¬ë¶„ ê°€ëŠ¥í•˜ë‚˜ í˜„ì¬ëŠ” deep ëª¨ë¸ ê¶Œì¥
            model_name = config.GROQ_MODEL_DEEP
            
            completion = groq_client.chat.completions.create(
                messages=[
                    {"role": "system", "content": "You are a financial analyst specializing in the Korean stock market. Output strictly in JSON."},
                    {"role": "user", "content": full_prompt}
                ],
                model=model_name,
                temperature=0.1,
                response_format={"type": "json_object"} # JSON ëª¨ë“œ
            )
            text = completion.choices[0].message.content
            
        else:
            # Gemini í˜¸ì¶œ
            model = genai.GenerativeModel(config.GEMINI_MODEL)
            response = model.generate_content(full_prompt)
            text = response.text

        # JSON íŒŒì‹± ê³µí†µ ë¡œì§
        text = text.strip()
        if text.startswith("```"):
            parts = text.split("```")
            if len(parts) >= 2:
                text = parts[1]
                if text.startswith("json"):
                    text = text[4:]
        text = text.strip()
        
        # Groqì˜ ê²½ìš° response_formatì„ ì¨ë„ ê°€ë” ë˜í•‘ëœ JSONì´ ì˜¬ ìˆ˜ ìˆìŒ
        try:
            results_list = json.loads(text)
            # ë§Œì•½ {"articles": [...]} í˜•íƒœë¼ë©´ ì¶”ì¶œ
            if isinstance(results_list, dict):
                for key in results_list:
                    if isinstance(results_list[key], list):
                        results_list = results_list[key]
                        break
        except json.JSONDecodeError:
            # ì¬ì‹œë„ ë˜ëŠ” ì˜¤ë¥˜ ì²˜ë¦¬
            raise ValueError(f"JSON íŒŒì‹± ì‹¤íŒ¨. ì‘ë‹µ ë‚´ìš©: {text[:100]}...")
            
        
        # ê²°ê³¼ ë§¤í•‘
        analyzed_articles = []
        
        # IDë¡œ ë§¤í•‘í•˜ê¸° ìœ„í•´ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        results_map = {}
        if isinstance(results_list, list):
            for res in results_list:
                if "id" in res:
                    results_map[res["id"]] = res
                    
        for idx, article in enumerate(articles):
            article_id = f"news_{idx}"
            result = results_map.get(article_id)
            
            if result:
                clean_result = {
                    "korea_relevance": result.get("korea_relevance", "none"),
                    "impact_direction": result.get("impact_direction", "neutral"),
                    "confidence": float(result.get("confidence", 0.5)),
                    "affected_sectors": result.get("affected_sectors", []),
                    "reasoning": result.get("reasoning", ""),
                    "title_ko": result.get("title_ko", ""),
                    "analysis_method": "groq" if use_groq else "gemini"
                }
                
                if clean_result["korea_relevance"] not in ["high", "medium", "low", "none"]:
                    clean_result["korea_relevance"] = "none"
                    
                article["korea_impact"] = clean_result
            else:
                logger.warning(f"ë°°ì¹˜ ë¶„ì„ ëˆ„ë½: {article.get('title')}")
                article["korea_impact"] = analyze_korea_impact_fallback(article)
                
            analyzed_articles.append(article)
            
        return analyzed_articles

    except Exception as e:
        logger.error(f"{'Groq' if use_groq else 'Gemini'} ë°°ì¹˜ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return _fallback_batch(articles)


def _fallback_batch(articles: List[Dict]) -> List[Dict]:
    """ì „ì²´ ê·œì¹™ ê¸°ë°˜ ë¶„ì„ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜"""
    fallback_results = []
    for article in articles:
        article["korea_impact"] = analyze_korea_impact_fallback(article)
        fallback_results.append(article)
    return fallback_results


def analyze_korea_impact_fallback(article: Dict) -> Dict:
    """
    ê·œì¹™ ê¸°ë°˜ í•œêµ­ ì‹œì¥ ì˜í–¥ ë¶„ì„ (Gemini ì‹¤íŒ¨ ì‹œ í´ë°±).
    """
    from collectors.finance_rss import detect_affected_sectors, calculate_korea_relevance
    
    title = article.get('title', '')
    description = article.get('description', '')
    
    sectors = detect_affected_sectors(title, description)
    relevance = calculate_korea_relevance(title, description)
    
    # ê°ì • ë¶„ì„ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)
    text_lower = f"{title} {description}".lower()
    
    negative_words = ["drop", "fall", "decline", "crash", "risk", "concern", 
                      "warning", "cut", "ban", "restrict", "sanction", "loss"]
    positive_words = ["rise", "gain", "surge", "boost", "strong", "growth",
                      "profit", "deal", "invest", "expand", "record"]
    
    neg_count = sum(1 for w in negative_words if w in text_lower)
    pos_count = sum(1 for w in positive_words if w in text_lower)
    
    if neg_count > pos_count:
        direction = "negative"
    elif pos_count > neg_count:
        direction = "positive"
    else:
        direction = "neutral"
    
    return {
        "korea_relevance": relevance,
        "impact_direction": direction,
        "confidence": 0.5,  # ê·œì¹™ ê¸°ë°˜ì€ ë‚®ì€ confidence
        "affected_sectors": sectors,
        "reasoning": "",
        "title_ko": "",
        "analysis_method": "rule_based"
    }


def get_recommended_stocks(sectors: List[str], direction: str) -> List[Dict]:
    """
    ì˜í–¥ë°›ëŠ” ì„¹í„°ì—ì„œ ì¢…ëª©ì„ ì¶”ì²œí•©ë‹ˆë‹¤.
    
    Args:
        sectors: ì˜í–¥ë°›ëŠ” ì„¹í„° ëª©ë¡
        direction: ì˜í–¥ ë°©í–¥ (positive/negative/neutral)
        
    Returns:
        List[Dict]: ì¶”ì²œ ì¢…ëª© ëª©ë¡
    """
    recommendations = []
    
    for sector in sectors:
        if sector not in SECTOR_STOCKS:
            continue
        
        stocks = SECTOR_STOCKS[sector]
        
        for stock in stocks:
            # ë†’ì€ ê°€ì¤‘ì¹˜ ì¢…ëª©ë§Œ ì¶”ì²œ
            if stock["weight"] in ["high", "medium"]:
                recommendations.append({
                    "code": stock["code"],
                    "name": stock["name"],
                    "sector": sector,
                    "direction": direction,
                    "weight": stock["weight"]
                })
    
    # ì¤‘ë³µ ì œê±° (ì¢…ëª© ì½”ë“œ ê¸°ì¤€)
    seen = set()
    unique_recs = []
    for rec in recommendations:
        if rec["code"] not in seen:
            seen.add(rec["code"])
            unique_recs.append(rec)
    
    # ê°€ì¤‘ì¹˜ ìˆœ ì •ë ¬
    weight_order = {"high": 0, "medium": 1, "low": 2}
    unique_recs.sort(key=lambda x: weight_order.get(x["weight"], 2))
    
    return unique_recs[:10]  # ìµœëŒ€ 10ê°œ



def analyze_news_batch(articles: List[Dict], use_ai: bool = True, rate_limit_delay: float = 20.0) -> List[Dict]:
    """
    ì—¬ëŸ¬ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë°°ì¹˜ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        articles: ë¶„ì„í•  ê¸°ì‚¬ ëª©ë¡
        use_ai: AI ë¶„ì„ ì‚¬ìš© ì—¬ë¶€ (Falseë©´ ê·œì¹™ ê¸°ë°˜ë§Œ ì‚¬ìš©)
        rate_limit_delay: ë°°ì¹˜ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ) - ë¬´ë£Œ í• ë‹¹ëŸ‰ ë³´í˜¸ë¥¼ ìœ„í•´ ì¶©ë¶„íˆ ì„¤ì •
        
    Returns:
        List[Dict]: ë¶„ì„ ê²°ê³¼ê°€ ì¶”ê°€ëœ ê¸°ì‚¬ ëª©ë¡
    """
    import time
    
    logger.info(f"ğŸ“Š í•œêµ­ ì‹œì¥ ì˜í–¥ ë¶„ì„ ì‹œì‘: {len(articles)}ê°œ ê¸°ì‚¬")
    
    analyzed_final = []
    korea_related_count = 0
    
    # ë°°ì¹˜ ì„¤ì • (Gemini Context Window ê³ ë ¤í•˜ì—¬ 5~10ê°œ ì ì ˆ)
    BATCH_SIZE = 10 
    
    # AI ë¯¸ì‚¬ìš© ì‹œ ì „ì²´ ê·œì¹™ ê¸°ë°˜ ì²˜ë¦¬
    if not use_ai or not GEMINI_AVAILABLE:
        logger.info("â„¹ï¸ AI ë¯¸ì‚¬ìš© ë˜ëŠ” Gemini ëª¨ë“ˆ ë¶€ì¬ - ì „ì²´ ê·œì¹™ ê¸°ë°˜ ë¶„ì„")
        for article in articles:
            impact = analyze_korea_impact_fallback(article)
            article["korea_impact"] = impact
            analyzed_final.append(article)
            if impact.get("korea_relevance") in ["high", "medium"]:
                korea_related_count += 1
        return analyzed_final

    # ë°°ì¹˜ ì²˜ë¦¬ ë£¨í”„
    total_batches = (len(articles) + BATCH_SIZE - 1) // BATCH_SIZE
    
    for i in range(0, len(articles), BATCH_SIZE):
        batch = articles[i:i + BATCH_SIZE]
        current_batch_num = (i // BATCH_SIZE) + 1
        
        logger.info(f"   ğŸ”„ Batch {current_batch_num}/{total_batches} ë¶„ì„ ì¤‘ ({len(batch)}ê±´)...")
        
        try:
            # ë°°ì¹˜ ë¶„ì„ ì‹¤í–‰
            analyzed_batch = analyze_korea_impact_batch(batch, mode=config.AI_ANALYZER_MODE)
            
            # í›„ì²˜ë¦¬ (ì¶”ì²œ ì¢…ëª© ë“±) ë° ê²°ê³¼ ì§‘ê³„
            for article in analyzed_batch:
                impact = article.get("korea_impact", {})
                
                if impact.get("korea_relevance") in ["high", "medium"]:
                    korea_related_count += 1
                    
                    # ì¶”ì²œ ì¢…ëª© ì¶”ê°€
                    if impact.get("affected_sectors"):
                        article["recommended_stocks"] = get_recommended_stocks(
                            impact["affected_sectors"],
                            impact.get("impact_direction", "neutral")
                        )
                
                analyzed_final.append(article)
            
            # Rate Limiting (ë§ˆì§€ë§‰ ë°°ì¹˜ ì œì™¸)
            if i + BATCH_SIZE < len(articles):
                logger.debug(f"   â³ Rate limit ëŒ€ê¸° ({rate_limit_delay}ì´ˆ)...")
                time.sleep(rate_limit_delay)
                
        except Exception as e:
            logger.error(f"   âŒ Batch {current_batch_num} ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ í•´ë‹¹ ë°°ì¹˜ë§Œ í´ë°± ì²˜ë¦¬í•˜ì—¬ ì§„í–‰
            for article in batch:
                article["korea_impact"] = analyze_korea_impact_fallback(article)
                analyzed_final.append(article)

    logger.info(f"   âœ… ë¶„ì„ ì™„ë£Œ: {len(analyzed_final)}ê°œ ì¤‘ {korea_related_count}ê°œ í•œêµ­ ê´€ë ¨")
    return analyzed_final


def filter_high_impact_news(articles: List[Dict]) -> List[Dict]:
    """
    í•œêµ­ ì‹œì¥ì— ë†’ì€ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ë‰´ìŠ¤ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
    """
    high_impact = []
    
    for article in articles:
        impact = article.get("korea_impact", {})
        relevance = impact.get("korea_relevance", "none")
        
        if relevance in ["high", "medium"]:
            high_impact.append(article)
    
    # confidence ìˆœ ì •ë ¬
    high_impact.sort(
        key=lambda x: x.get("korea_impact", {}).get("confidence", 0),
        reverse=True
    )
    
    return high_impact


def format_impact_report(articles: List[Dict]) -> str:
    """
    í•œêµ­ ì‹œì¥ ì˜í–¥ ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.
    """
    high_impact = filter_high_impact_news(articles)
    
    if not high_impact:
        return "ğŸ“­ í•œêµ­ ì‹œì¥ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì£¼ìš” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    lines = [
        "ğŸ“° **í•œêµ­ ì‹œì¥ ì˜í–¥ ë‰´ìŠ¤**",
        f"ì´ {len(high_impact)}ê±´ì˜ ê´€ë ¨ ë‰´ìŠ¤",
        ""
    ]
    
    for i, article in enumerate(high_impact[:10], 1):
        impact = article.get("korea_impact", {})
        direction = impact.get("impact_direction", "neutral")
        
        emoji = {"positive": "ğŸŸ¢", "negative": "ğŸ”´", "neutral": "âšª"}.get(direction, "âšª")
        
        title_ko = impact.get("title_ko") or article.get("title", "")[:50]
        sectors = ", ".join(impact.get("affected_sectors", [])[:3])
        reasoning = impact.get("reasoning", "")[:80]
        
        lines.append(f"{i}. {emoji} **{title_ko}**")
        if sectors:
            lines.append(f"   ì˜í–¥ ì„¹í„°: {sectors}")
        if reasoning:
            lines.append(f"   â†’ {reasoning}")
        
        # ì¶”ì²œ ì¢…ëª©
        stocks = article.get("recommended_stocks", [])[:3]
        if stocks:
            stock_names = ", ".join([s["name"] for s in stocks])
            lines.append(f"   ğŸ’¡ ê´€ë ¨ ì¢…ëª©: {stock_names}")
        
        lines.append("")
    
    return "\n".join(lines)


# í…ŒìŠ¤íŠ¸ìš©
if __name__ == "__main__":
    # ìƒ˜í”Œ ê¸°ì‚¬ë¡œ í…ŒìŠ¤íŠ¸
    sample_articles = [
        {
            "title": "Samsung Electronics Q4 profit drops 30% amid chip downturn",
            "description": "Samsung Electronics reported a 30% decline in fourth-quarter operating profit due to weak memory chip demand.",
            "source": "WSJ"
        },
        {
            "title": "Tesla to expand battery production with new suppliers",
            "description": "Tesla announced plans to significantly expand its battery supply chain, potentially benefiting Asian suppliers.",
            "source": "BBC Business"
        },
        {
            "title": "Fed signals potential rate cut in upcoming meeting",
            "description": "The Federal Reserve indicated it may reduce interest rates, boosting global market sentiment.",
            "source": "NPR Business"
        }
    ]
    
    print("í•œêµ­ ì‹œì¥ ì˜í–¥ ë¶„ì„ í…ŒìŠ¤íŠ¸...")
    analyzed = analyze_news_batch(sample_articles)
    print(format_impact_report(analyzed))
