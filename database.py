"""
ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë“ˆ (Supabase)
ëª¨ë“  DB ì‘ì—…ì„ ì¤‘ì•™ì—ì„œ ê´€ë¦¬í•©ë‹ˆë‹¤.
"""
import os
import json
from datetime import datetime, timezone, timedelta
from dateutil import parser as date_parser
from supabase import create_client, Client
from typing import List, Dict, Optional, Set, Tuple
from collections import defaultdict
import config

# ë¡œê±° ì„¤ì •
logger = config.setup_logger(__name__)

# ì‹¤íŒ¨ ê¸°ì‚¬ ê¸°ë¡ íŒŒì¼
FAILED_ARTICLES_LOG = os.path.join(config.LOG_DIR, "failed_articles.jsonl")

# ==============================================
# Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# ==============================================
supabase: Optional[Client] = None

try:
    if not config.SUPABASE_URL or not config.SUPABASE_KEY:
        raise ValueError("Supabase ì„¤ì • ëˆ„ë½: SUPABASE_URL ë˜ëŠ” SUPABASE_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")

    supabase = create_client(config.SUPABASE_URL, config.SUPABASE_KEY)
    logger.info("âœ… Supabase ì—°ê²° ì„±ê³µ")
except ValueError as e:
    logger.error(f"âŒ Supabase ì„¤ì • ì˜¤ë¥˜: {e}")
    supabase = None
except Exception as e:
    logger.error(f"âŒ Supabase ì—°ê²° ì‹¤íŒ¨: {e}")
    supabase = None

# ==============================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ==============================================
def ensure_connection():
    """DB ì—°ê²° í™•ì¸"""
    if not supabase:
        raise RuntimeError("Supabase í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    return supabase


def validate_article(article: Dict) -> Tuple[bool, str]:
    """
    ê¸°ì‚¬ ë°ì´í„° ê²€ì¦ ë° ì •ê·œí™”
    
    Args:
        article: ê²€ì¦í•  ê¸°ì‚¬ ë°ì´í„°
        
    Returns:
        Tuple[bool, str]: (ì„±ê³µ ì—¬ë¶€, ì—ëŸ¬ ë©”ì‹œì§€)
    """
    # 1. í•„ìˆ˜ í•„ë“œ ì •ì˜
    required_fields = ['title', 'link', 'source']
    
    # 2. í•„ìˆ˜ í•„ë“œ ì²´í¬
    missing = [f for f in required_fields if f not in article or not article[f]]
    if missing:
        return False, f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {', '.join(missing)}"
        
    # 3. ë‚ ì§œ í•„ë“œ í˜¸í™˜ì„± ë° ì •ê·œí™” (í•µì‹¬ ìˆ˜ì • ì‚¬í•­)
    # Streamlit ë“±ì—ì„œ ì—ëŸ¬ê°€ ë‚˜ì§€ ì•Šë„ë¡ ì €ì¥ ì „ì— ISO í¬ë§·ìœ¼ë¡œ í†µì¼í•©ë‹ˆë‹¤.
    try:
        if 'pub_date' in article and 'published_at' not in article:
            article['published_at'] = article.pop('pub_date')
            
        if 'published_at' in article:
            date_val = article['published_at']
            # ë¬¸ìì—´ì´ë©´ íŒŒì‹± í›„ ë‹¤ì‹œ í¬ë§·íŒ…
            if isinstance(date_val, str):
                dt = date_parser.parse(date_val)
            elif isinstance(date_val, datetime):
                dt = date_val
            else:
                dt = datetime.now(timezone.utc)
            
            # íƒ€ì„ì¡´ì´ ì—†ìœ¼ë©´ UTCë¡œ ê°„ì£¼
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            
            # í‘œì¤€ ISO 8601 ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
            article['published_at'] = dt.isoformat()
        else:
            # ë‚ ì§œê°€ ì—†ìœ¼ë©´ í˜„ì¬ ì‹œê°„
            article['published_at'] = datetime.now(timezone.utc).isoformat()
            
    except Exception as e:
        # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ëŒ€ì²´ (ë°ì´í„° ìœ ì‹¤ ë°©ì§€)
        logger.warning(f"ë‚ ì§œ í˜•ì‹ ë³€í™˜ ì‹¤íŒ¨ ({article.get('published_at')}): {e}, í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ëŒ€ì²´")
        article['published_at'] = datetime.now(timezone.utc).isoformat()
        
    return True, "Valid"


def log_failed_article(article: Dict, error_reason: str, error_type: str = "SAVE_FAILED"):
    """
    ì‹¤íŒ¨í•œ ê¸°ì‚¬ë¥¼ ìƒì„¸ ì •ë³´ì™€ í•¨ê»˜ ê¸°ë¡
    """
    try:
        os.makedirs(config.LOG_DIR, exist_ok=True)
        
        failed_record = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "error_type": error_type,
            "error_reason": error_reason,
            "article": {
                "source": article.get('source', 'N/A'),
                "title": article.get('title', 'N/A'),
                "link": article.get('link', 'N/A'),
                "published_at": article.get('published_at', 'N/A')
            }
        }
        
        with open(FAILED_ARTICLES_LOG, 'a', encoding='utf-8') as f:
            f.write(json.dumps(failed_record, ensure_ascii=False) + '\n')
            
    except Exception as e:
        logger.warning(f"ì‹¤íŒ¨ ê¸°ì‚¬ ê¸°ë¡ ì‹¤íŒ¨: {e}")


# ==============================================
# ì¡°íšŒ í•¨ìˆ˜ë“¤ (Getters)
# ==============================================

def get_keywords() -> List[str]:
    """DBì—ì„œ í™œì„±í™”ëœ í‚¤ì›Œë“œ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        db = ensure_connection()
        response = db.table("keywords").select("word").eq("enabled", True).execute()
        keywords = [item['word'] for item in response.data]
        return keywords
    except RuntimeError:
        logger.warning("DB ë¯¸ì—°ê²° - configì˜ ê¸°ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©")
        return config.KEYWORDS_EN + config.KEYWORDS_KO
    except Exception as e:
        logger.error(f"âŒ í‚¤ì›Œë“œ ë¡œë”© ì‹¤íŒ¨: {e}")
        return config.KEYWORDS_EN + config.KEYWORDS_KO


def get_countries() -> List[Dict]:
    """DBì—ì„œ í™œì„±í™”ëœ êµ­ê°€ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        db = ensure_connection()
        response = db.table("countries").select("*").eq("enabled", True).execute()
        return response.data
    except RuntimeError:
        logger.warning("DB ë¯¸ì—°ê²° - ê¸°ë³¸ êµ­ê°€ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©")
        return [{"code": "Global", "name": "Global", "enabled": True}]
    except Exception as e:
        logger.error(f"âŒ êµ­ê°€ ì •ë³´ ë¡œë”© ì‹¤íŒ¨: {e}")
        return []


def get_ban_words() -> List[str]:
    """DBì—ì„œ í™œì„±í™”ëœ ê¸ˆì§€ì–´ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        db = ensure_connection()
        response = db.table("ban_words").select("word").eq("enabled", True).execute()
        ban_words = [item['word'] for item in response.data]
        return ban_words
    except Exception as e:
        logger.error(f"âŒ ê¸ˆì§€ì–´ ë¡œë”© ì‹¤íŒ¨: {e}")
        return []


def get_monitored_sites() -> List[Dict]:
    """í™œì„±í™”ëœ ì»¤ìŠ¤í…€ RSS ì‚¬ì´íŠ¸ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        db = ensure_connection()
        response = db.table("monitored_sites").select("*").eq("enabled", True).execute()
        return response.data
    except Exception as e:
        logger.error(f"âŒ RSS ì‚¬ì´íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return []


def get_recent_links(days: int = 2) -> Set[str]:
    """ì¤‘ë³µ ë°©ì§€ìš©: ìµœê·¼ Nì¼ ê°„ì˜ ê¸°ì‚¬ ë§í¬ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        db = ensure_connection()
        cutoff = (datetime.now(timezone.utc) - timedelta(days=days)).isoformat()
        
        response = db.table("news")\
            .select("link")\
            .gte("created_at", cutoff)\
            .execute()
            
        links = {item['link'] for item in response.data}
        return links
        
    except Exception as e:
        logger.error(f"âŒ ìµœê·¼ ë§í¬ ë¡œë”© ì‹¤íŒ¨: {e}")
        return set()


def get_unprocessed_articles(limit: int = 10, days: int = 3) -> List[Dict]:
    """AI ë¶„ì„ì´ í•„ìš”í•œ ê¸°ì‚¬ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        db = ensure_connection()
        cutoff = (datetime.now(timezone.utc) - timedelta(days=days)).isoformat()
        
        response = db.table("news")\
            .select("*")\
            .gte("created_at", cutoff)\
            .is_("summary_ai", "null")\
            .limit(limit)\
            .execute()
            
        return response.data
    except Exception as e:
        logger.error(f"âŒ ë¯¸ì²˜ë¦¬ ê¸°ì‚¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []


def get_recent_articles(hours: int = 24) -> List[Dict]:
    """ë¦¬í¬íŠ¸ ìƒì„±ìš©: ìµœê·¼ Nì‹œê°„ ë‚´ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        db = ensure_connection()
        cutoff = (datetime.now(timezone.utc) - timedelta(hours=hours)).isoformat()
        
        response = db.table("news")\
            .select("*")\
            .gte("created_at", cutoff)\
            .execute()
            
        return response.data
    except Exception as e:
        logger.error(f"âŒ ìµœê·¼ ê¸°ì‚¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []

# ==============================================
# ì„¤ì • ë° ìš´ì˜ ë¡œì§ (Settings)
# ==============================================

def get_settings() -> Dict:
    """ìˆ˜ì§‘ ì„¤ì • ê°€ì ¸ì˜¤ê¸°"""
    default_settings = {
        "interval_minutes": config.DEFAULT_COLLECTION_INTERVAL,
        "last_run": "2000-01-01T00:00:00+00:00"
    }
    
    try:
        db = ensure_connection()
        response = db.table("settings").select("*").limit(1).single().execute()
        return response.data
    except Exception:
        return default_settings


def update_last_run() -> bool:
    """ë§ˆì§€ë§‰ ì‹¤í–‰ ì‹œê°„ ì—…ë°ì´íŠ¸"""
    try:
        db = ensure_connection()
        now = datetime.now(timezone.utc).isoformat()
        settings = get_settings()
        target_id = settings.get('id', 1)
        
        db.table("settings").update({
            "last_run": now,
            "updated_at": now
        }).eq("id", target_id).execute()
        return True
    except Exception as e:
        logger.error(f"âŒ ë§ˆì§€ë§‰ ì‹¤í–‰ ì‹œê°„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        return False


def should_run_collector() -> bool:
    """ìˆ˜ì§‘ ì£¼ê¸° ì²´í¬"""
    try:
        settings = get_settings()
        last_run_str = settings.get('last_run')
        
        try:
            last_run = date_parser.parse(last_run_str)
            if last_run.tzinfo is None:
                last_run = last_run.replace(tzinfo=timezone.utc)
        except:
            last_run = datetime(2000, 1, 1, tzinfo=timezone.utc)
            
        interval_minutes = int(settings.get('interval_minutes', config.DEFAULT_COLLECTION_INTERVAL))
        elapsed_minutes = (datetime.now(timezone.utc) - last_run).total_seconds() / 60
        
        if elapsed_minutes >= interval_minutes:
            logger.info(f"âœ… ìˆ˜ì§‘ ì£¼ê¸° ë„ë˜ (ê²½ê³¼: {elapsed_minutes:.1f}ë¶„)")
            return True
        else:
            logger.info(f"â³ ìˆ˜ì§‘ ì£¼ê¸° ë¯¸ë„ë˜ (ë‚¨ì€ì‹œê°„: {interval_minutes - elapsed_minutes:.0f}ë¶„)")
            return False
            
    except Exception:
        return True

# ==============================================
# ì €ì¥ í•¨ìˆ˜ (ë¶„í•  ì €ì¥)
# ==============================================

def save_news_batch(news_list: List[Dict]) -> int:
    """
    ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì €ì¥í•©ë‹ˆë‹¤.
    """
    if not news_list:
        return 0
    
    try:
        db = ensure_connection()
    except RuntimeError:
        return 0
    
    saved_count = 0
    duplicate_count = 0
    validation_failed_count = 0
    save_failed_count = 0
    chunk_size = config.COLLECTOR_BATCH_SIZE
    total_chunks = (len(news_list) + chunk_size - 1) // chunk_size
    
    logger.info(f"ğŸ’¾ ì´ {len(news_list)}ê°œ ê¸°ì‚¬ë¥¼ {total_chunks}ê°œ ì²­í¬ë¡œ ë¶„í•  ì €ì¥ ì‹œì‘")
    
    for i in range(0, len(news_list), chunk_size):
        chunk = news_list[i:i + chunk_size]
        chunk_num = (i // chunk_size) + 1
        
        try:
            # 1. ë°ì´í„° ê²€ì¦ ë° ì •ê·œí™”
            validation_failed = []
            articles_to_check = []
            
            for idx, article in enumerate(chunk):
                is_valid, error_msg = validate_article(article)
                if not is_valid:
                    validation_failed.append((idx, article, error_msg))
                    log_failed_article(article, error_msg, "VALIDATION_FAILED")
                else:
                    articles_to_check.append(article)
            
            validation_failed_count += len(validation_failed)
            
            if not articles_to_check:
                continue
            
            # 2. ì¤‘ë³µ í™•ì¸
            chunk_links = [article['link'] for article in articles_to_check]
            try:
                existing = db.table("news").select("link").in_("link", chunk_links).execute()
                existing_links = {item['link'] for item in existing.data}
            except:
                existing_links = set()
            
            # 3. ì‹ ê·œ ê¸°ì‚¬ í•„í„°ë§
            new_news = [article for article in articles_to_check if article['link'] not in existing_links]
            current_dupes = len(articles_to_check) - len(new_news)
            duplicate_count += current_dupes
            
            # 4. DB ì €ì¥
            if new_news:
                try:
                    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
                    allowed_columns = {
                        'title', 'link', 'description', 'published_at', 'source', 'country',
                        'author', 'content_hash', 'is_processed', 'collected_at',
                        'summary_ai', 'title_ko', 'category', 'sentiment', 'quality_score'
                    }
                    cleaned_batch = []
                    for art in new_news:
                        cleaned = {k: v for k, v in art.items() if k in allowed_columns}
                        cleaned_batch.append(cleaned)

                    result = db.table("news").insert(cleaned_batch).execute()
                    saved_count += len(result.data)
                    logger.info(f"   [{chunk_num}/{total_chunks}] âœ… {len(result.data)}ê°œ ì €ì¥ ì™„ë£Œ")
                    
                except Exception as insert_error:
                    logger.warning(f"   [{chunk_num}/{total_chunks}] âš ï¸ ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨ -> ê°œë³„ ì €ì¥ ì‹œë„")
                    
                    # ê°œë³„ ì €ì¥ ëª¨ë“œ (Fallback)
                    chunk_saved = 0
                    for article in new_news:
                        try:
                            cleaned = {k: v for k, v in article.items() if k in allowed_columns}
                            db.table("news").insert([cleaned]).execute()
                            chunk_saved += 1
                        except Exception as e:
                            log_failed_article(article, str(e), "INSERT_FAILED")
                    
                    saved_count += chunk_saved
                    save_failed_count += (len(new_news) - chunk_saved)
            
        except Exception as e:
            logger.error(f"   [{chunk_num}/{total_chunks}] âŒ ì²­í¬ ì—ëŸ¬: {e}")
            save_failed_count += len(chunk)
            continue
            
    return saved_count


def update_article_analysis(article_id: int, analysis_result: Dict) -> bool:
    """
    ê¸°ì‚¬ì˜ AI ë¶„ì„ ê²°ê³¼ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    """
    try:
        db = ensure_connection()
        
        # analyzer.pyëŠ” 'summary'ë¥¼ ì£¼ì§€ë§Œ, DB ì»¬ëŸ¼ì€ 'summary_ai'ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
        update_data = {
            "title_ko": analysis_result.get("title_ko"),
            "summary_ai": analysis_result.get("summary"),  # ë§¤í•‘ ì²˜ë¦¬
            "category": analysis_result.get("category"),
            "sentiment": analysis_result.get("sentiment"),
            "is_processed": True,
            "quality_score": analysis_result.get("quality_score", 0)
        }
        
        db.table("news").update(update_data).eq("id", article_id).execute()
        return True
        
    except Exception as e:
        logger.error(f"âŒ ë¶„ì„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (ID: {article_id}): {e}")
        return False


def save_daily_report(report_date: str, content: str, keywords: List[str]) -> bool:
    """ì¼ì¼ ë¦¬í¬íŠ¸ ì €ì¥"""
    try:
        db = ensure_connection()
        db.table("daily_reports").insert({
            "report_date": report_date,
            "content": content,
            "keywords": keywords
        }).execute()
        return True
    except Exception as e:
        logger.error(f"âŒ ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

# ==============================================
# í†µê³„ ì¡°íšŒ
# ==============================================

def get_statistics(days: int = 7) -> Dict:
    """í†µê³„ ì¡°íšŒ"""
    try:
        db = ensure_connection()
        cutoff = (datetime.now(timezone.utc) - timedelta(days=days)).isoformat()
        
        total = db.table("news").select("id", count="exact").gte("created_at", cutoff).execute()
        processed = db.table("news").select("id", count="exact")\
            .gte("created_at", cutoff).eq("is_processed", True).execute()
        
        # ì†ŒìŠ¤ë³„ í†µê³„ëŠ” ë°ì´í„°ê°€ ë§ìœ¼ë©´ ëŠë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì˜ˆì™¸ì²˜ë¦¬
        try:
            sources_resp = db.table("news").select("source")\
                .gte("created_at", cutoff).execute()
            # Python ì¸¡ì—ì„œ ì¹´ìš´íŒ… (Supabase GroupByê°€ ë³µì¡í•˜ë¯€ë¡œ)
            source_counts = defaultdict(int)
            for item in sources_resp.data:
                source_counts[item.get('source', 'Unknown')] += 1
            sources_data = [{"source": k, "count": v} for k, v in source_counts.items()]
        except:
            sources_data = []
        
        return {
            "total_articles": total.count,
            "processed_articles": processed.count,
            "unprocessed_articles": total.count - processed.count,
            "processing_rate": round((processed.count / total.count * 100) if total.count > 0 else 0, 1),
            "sources": sources_data
        }
    except Exception as e:
        logger.error(f"í†µê³„ ì—ëŸ¬: {e}")
        return {"total_articles": 0, "processed_articles": 0, "processing_rate": 0, "sources": []}

if __name__ == "__main__":
    # ê°„ë‹¨ í…ŒìŠ¤íŠ¸
    if ensure_connection():
        print("âœ… DB ì—°ê²° ì •ìƒ")
        print(f"   í‚¤ì›Œë“œ: {len(get_keywords())}ê°œ")
        print(f"   êµ­ê°€: {len(get_countries())}ê°œ")