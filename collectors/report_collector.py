"""
íˆ¬ì ë¦¬í¬íŠ¸ ìˆ˜ì§‘ê¸° (v1.0)
- í•´ì™¸ íˆ¬ìì€í–‰/ê¸°ê´€ ë¦¬í¬íŠ¸ (RSS)
- êµ­ë‚´ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ (ë„¤ì´ë²„ ì¦ê¶Œ)
- ê²½ì œì§€í‘œ ìº˜ë¦°ë”
"""
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Optional
import feedparser
import re

import config

logger = config.setup_logger(__name__)


# ==============================================
# í•´ì™¸ íˆ¬ìì€í–‰ ë¦¬í¬íŠ¸ RSS ì†ŒìŠ¤
# ==============================================
IB_REPORT_SOURCES = [
    # Bloomberg
    {
        "url": "https://feeds.bloomberg.com/markets/news.rss",
        "name": "Bloomberg Markets",
        "category": "IB",
        "country": "US"
    },
    # Reuters
    {
        "url": "https://www.reutersagency.com/feed/?taxonomy=best-sectors&post_type=best",
        "name": "Reuters Analysis",
        "category": "IB",
        "country": "US"
    },
    # CNBC
    {
        "url": "https://www.cnbc.com/id/10001147/device/rss/rss.html",
        "name": "CNBC Investing",
        "category": "IB",
        "country": "US"
    },
    # Financial Times
    {
        "url": "https://www.ft.com/markets?format=rss",
        "name": "FT Markets",
        "category": "IB",
        "country": "UK"
    },
    # MarketWatch
    {
        "url": "https://feeds.marketwatch.com/marketwatch/topstories/",
        "name": "MarketWatch",
        "category": "IB",
        "country": "US"
    }
]

# ë¦¬í¬íŠ¸ ê´€ë ¨ í‚¤ì›Œë“œ (í•„í„°ë§ìš©)
REPORT_KEYWORDS = [
    # ì˜ë¬¸
    "upgrade", "downgrade", "target price", "price target", "rating",
    "buy", "sell", "hold", "outperform", "underperform",
    "analyst", "forecast", "outlook", "earnings", "guidance",
    "goldman", "morgan", "jp morgan", "citi", "ubs", "hsbc",
    "merrill", "barclays", "credit suisse", "deutsche bank",
    # í•œê¸€
    "ëª©í‘œê°€", "íˆ¬ìì˜ê²¬", "ë§¤ìˆ˜", "ë§¤ë„", "ì¤‘ë¦½", "ë¹„ì¤‘í™•ëŒ€", "ë¹„ì¤‘ì¶•ì†Œ",
    "ìƒí–¥", "í•˜í–¥", "ë¦¬í¬íŠ¸", "ì• ë„ë¦¬ìŠ¤íŠ¸", "ì „ë§"
]


def fetch_ib_reports(hours: int = 24) -> List[Dict]:
    """
    í•´ì™¸ íˆ¬ìì€í–‰/ê¸°ê´€ ë¦¬í¬íŠ¸ ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘
    
    Args:
        hours: ìˆ˜ì§‘ ê¸°ê°„ (ì‹œê°„)
        
    Returns:
        List[Dict]: ë¦¬í¬íŠ¸ ë‰´ìŠ¤ ëª©ë¡
    """
    logger.info("ğŸ“Š í•´ì™¸ íˆ¬ìì€í–‰ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì‹œì‘...")
    reports = []
    cutoff = datetime.now(timezone.utc) - timedelta(hours=hours)
    
    for source in IB_REPORT_SOURCES:
        try:
            feed = feedparser.parse(source["url"])
            
            for entry in feed.entries[:20]:  # ì†ŒìŠ¤ë‹¹ ìµœëŒ€ 20ê°œ
                title = entry.get("title", "")
                description = entry.get("description", entry.get("summary", ""))
                
                # ë¦¬í¬íŠ¸ ê´€ë ¨ í‚¤ì›Œë“œ í•„í„°ë§
                content = f"{title} {description}".lower()
                if not any(kw.lower() in content for kw in REPORT_KEYWORDS):
                    continue
                
                # ë‚ ì§œ íŒŒì‹±
                pub_date = None
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    pub_date = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    pub_date = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
                
                if pub_date and pub_date < cutoff:
                    continue
                
                reports.append({
                    "title": title,
                    "description": description[:500],
                    "link": entry.get("link", ""),
                    "source": source["name"],
                    "category": source["category"],
                    "country": source["country"],
                    "published_at": pub_date.isoformat() if pub_date else None,
                    "type": "IB_REPORT"
                })
                
            logger.info(f"   âœ… {source['name']}: ìˆ˜ì§‘ ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"   âš ï¸ {source['name']} ì‹¤íŒ¨: {e}")
    
    logger.info(f"ğŸ“Š í•´ì™¸ ë¦¬í¬íŠ¸ ì´ {len(reports)}ê±´ ìˆ˜ì§‘")
    return reports


def fetch_naver_reports(limit: int = 20) -> List[Dict]:
    """
    ë„¤ì´ë²„ ì¦ê¶Œ ë¦¬ì„œì¹˜ ë¦¬í¬íŠ¸ ìˆ˜ì§‘
    
    Args:
        limit: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
        
    Returns:
        List[Dict]: êµ­ë‚´ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ëª©ë¡
    """
    logger.info("ğŸ“ˆ ë„¤ì´ë²„ ì¦ê¶Œ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì‹œì‘...")
    reports = []
    
    # ë„¤ì´ë²„ ì¦ê¶Œ ë¦¬ì„œì¹˜ í˜ì´ì§€ë“¤
    pages = [
        ("https://finance.naver.com/research/company_list.naver", "ì¢…ëª©ë¶„ì„"),
        ("https://finance.naver.com/research/market_info_list.naver", "ì‹œì¥ë¶„ì„"),
        ("https://finance.naver.com/research/economy_list.naver", "ê²½ì œë¶„ì„"),
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    
    for url, category in pages:
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.encoding = 'euc-kr'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë¦¬í¬íŠ¸ í…Œì´ë¸” ì°¾ê¸°
            table = soup.find('table', class_='type_1')
            if not table:
                continue
            
            rows = table.find_all('tr')[2:]  # í—¤ë” ì œì™¸
            
            for row in rows[:limit // len(pages)]:
                cols = row.find_all('td')
                if len(cols) < 4:
                    continue
                
                # ì œëª©ê³¼ ë§í¬
                title_tag = cols[0].find('a')
                if not title_tag:
                    continue
                
                title = title_tag.get_text(strip=True)
                link = title_tag.get('href', '')
                if link and not link.startswith('http'):
                    link = f"https://finance.naver.com/research/{link}"
                
                # ì¦ê¶Œì‚¬
                firm = cols[1].get_text(strip=True) if len(cols) > 1 else "Unknown"
                
                # ë‚ ì§œ
                date_str = cols[-1].get_text(strip=True) if cols else ""
                
                reports.append({
                    "title": title,
                    "description": f"[{firm}] {title}",
                    "link": link,
                    "source": firm,
                    "category": category,
                    "country": "KR",
                    "published_at": date_str,
                    "type": "KR_REPORT"
                })
            
            logger.info(f"   âœ… {category}: ìˆ˜ì§‘ ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"   âš ï¸ {category} ì‹¤íŒ¨: {e}")
    
    logger.info(f"ğŸ“ˆ êµ­ë‚´ ë¦¬í¬íŠ¸ ì´ {len(reports)}ê±´ ìˆ˜ì§‘")
    return reports


def fetch_hankyung_consensus(limit: int = 10) -> List[Dict]:
    """
    í•œê²½ ì»¨ì„¼ì„œìŠ¤ ë¦¬í¬íŠ¸ ìˆ˜ì§‘
    
    Args:
        limit: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
        
    Returns:
        List[Dict]: ì»¨ì„¼ì„œìŠ¤ ë¦¬í¬íŠ¸ ëª©ë¡
    """
    logger.info("ğŸ“Š í•œê²½ ì»¨ì„¼ì„œìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
    reports = []
    
    url = "https://consensus.hankyung.com/apps.analysis/analysis.list"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë¦¬í¬íŠ¸ ë¦¬ìŠ¤íŠ¸ ì°¾ê¸°
        items = soup.find_all('div', class_='item')[:limit]
        
        for item in items:
            title_tag = item.find('a')
            if not title_tag:
                continue
            
            title = title_tag.get_text(strip=True)
            link = title_tag.get('href', '')
            if link and not link.startswith('http'):
                link = f"https://consensus.hankyung.com{link}"
            
            # ì¦ê¶Œì‚¬ ì •ë³´
            firm_tag = item.find('span', class_='firm')
            firm = firm_tag.get_text(strip=True) if firm_tag else "Unknown"
            
            reports.append({
                "title": title,
                "description": f"[{firm}] {title}",
                "link": link,
                "source": firm,
                "category": "ì»¨ì„¼ì„œìŠ¤",
                "country": "KR",
                "type": "KR_CONSENSUS"
            })
        
        logger.info(f"   âœ… í•œê²½ ì»¨ì„¼ì„œìŠ¤: {len(reports)}ê±´")
        
    except Exception as e:
        logger.warning(f"   âš ï¸ í•œê²½ ì»¨ì„¼ì„œìŠ¤ ì‹¤íŒ¨: {e}")
    
    return reports


def fetch_economic_calendar(days: int = 1) -> List[Dict]:
    """
    ì£¼ìš” ê²½ì œì§€í‘œ ì¼ì • ìˆ˜ì§‘ (Investing.com ìŠ¤íƒ€ì¼)
    
    Args:
        days: ìˆ˜ì§‘ ê¸°ê°„ (ì¼)
        
    Returns:
        List[Dict]: ê²½ì œì§€í‘œ ì¼ì • ëª©ë¡
    """
    logger.info("ğŸ“… ê²½ì œì§€í‘œ ìº˜ë¦°ë” ìˆ˜ì§‘ ì‹œì‘...")
    
    # ì£¼ìš” ê²½ì œì§€í‘œ (ìˆ˜ë™ ì •ì˜ - APIê°€ ì œí•œì ì´ë¯€ë¡œ)
    # ì‹¤ì œë¡œëŠ” Investing.com í¬ë¡¤ë§ ë˜ëŠ” FRED API ì‚¬ìš© ê¶Œì¥
    indicators = [
        {"name": "ë¯¸êµ­ CPI (ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜)", "importance": "high"},
        {"name": "ë¯¸êµ­ FOMC ê¸ˆë¦¬ê²°ì •", "importance": "high"},
        {"name": "ë¯¸êµ­ ê³ ìš©ì§€í‘œ (Non-Farm Payroll)", "importance": "high"},
        {"name": "ë¯¸êµ­ GDP ì„±ì¥ë¥ ", "importance": "high"},
        {"name": "í•œêµ­ ìˆ˜ì¶œì… ë™í–¥", "importance": "medium"},
        {"name": "ì¤‘êµ­ PMI (ì œì¡°ì—…êµ¬ë§¤ê´€ë¦¬ìì§€ìˆ˜)", "importance": "medium"},
    ]
    
    logger.info(f"   âœ… ì£¼ìš” ì§€í‘œ {len(indicators)}ê°œ ë¡œë“œ")
    return indicators


def collect_all_reports() -> Dict[str, List[Dict]]:
    """
    ëª¨ë“  ë¦¬í¬íŠ¸ ìˆ˜ì§‘ í†µí•© í•¨ìˆ˜
    
    Returns:
        Dict: ì¹´í…Œê³ ë¦¬ë³„ ë¦¬í¬íŠ¸ ëª©ë¡
    """
    logger.info("=" * 60)
    logger.info("ğŸ” íˆ¬ì ë¦¬í¬íŠ¸ ì¢…í•© ìˆ˜ì§‘ ì‹œì‘")
    logger.info("=" * 60)
    
    result = {
        "ib_reports": fetch_ib_reports(hours=24),
        "kr_reports": fetch_naver_reports(limit=20),
        "consensus": fetch_hankyung_consensus(limit=10),
        "economic_calendar": fetch_economic_calendar(days=1)
    }
    
    total = sum(len(v) for v in result.values())
    logger.info(f"ğŸ“Š ì´ {total}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
    
    return result


def format_reports_for_briefing(reports: Dict[str, List[Dict]]) -> str:
    """
    ë¸Œë¦¬í•‘ìš© ë¦¬í¬íŠ¸ ìš”ì•½ í¬ë§·íŒ…
    
    Args:
        reports: collect_all_reports() ê²°ê³¼
        
    Returns:
        str: ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ë¦¬í¬íŠ¸ ìš”ì•½
    """
    sections = []
    
    # í•´ì™¸ IB ë¦¬í¬íŠ¸
    ib = reports.get("ib_reports", [])
    if ib:
        ib_section = "## ğŸ“Š í•´ì™¸ íˆ¬ìì€í–‰ ë¦¬í¬íŠ¸\n\n"
        for r in ib[:5]:  # ìƒìœ„ 5ê°œ
            ib_section += f"- **{r['source']}**: {r['title'][:60]}...\n"
        sections.append(ib_section)
    
    # êµ­ë‚´ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸
    kr = reports.get("kr_reports", [])
    if kr:
        kr_section = "## ğŸ“ˆ êµ­ë‚´ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸\n\n"
        for r in kr[:5]:
            kr_section += f"- **[{r['source']}]** {r['title'][:50]}...\n"
        sections.append(kr_section)
    
    # ì»¨ì„¼ì„œìŠ¤
    consensus = reports.get("consensus", [])
    if consensus:
        con_section = "## ğŸ¯ í•œê²½ ì»¨ì„¼ì„œìŠ¤\n\n"
        for r in consensus[:3]:
            con_section += f"- {r['title'][:60]}...\n"
        sections.append(con_section)
    
    return "\n".join(sections) if sections else "ë¦¬í¬íŠ¸ ì •ë³´ ì—†ìŒ"


# ==============================================
# CLI í…ŒìŠ¤íŠ¸
# ==============================================
if __name__ == "__main__":
    print("ğŸ” ë¦¬í¬íŠ¸ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸")
    
    # í•´ì™¸ IB ë¦¬í¬íŠ¸
    ib = fetch_ib_reports(hours=24)
    print(f"\ní•´ì™¸ IB ë¦¬í¬íŠ¸: {len(ib)}ê±´")
    for r in ib[:3]:
        print(f"  - [{r['source']}] {r['title'][:50]}...")
    
    # êµ­ë‚´ ë¦¬í¬íŠ¸
    kr = fetch_naver_reports(limit=10)
    print(f"\nêµ­ë‚´ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸: {len(kr)}ê±´")
    for r in kr[:3]:
        print(f"  - [{r['source']}] {r['title'][:40]}...")
