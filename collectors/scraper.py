import time
from typing import List, Dict
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timezone
import urllib.parse
import config
import collector_utils as utils

logger = config.setup_logger(__name__)


def fetch_naver_news(keywords: List[str]) -> List[Dict]:
    """
    Naver ë‰´ìŠ¤ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ì—¬ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    logger.info("ğŸ“¡ [Naver News] ìˆ˜ì§‘ ì‹œì‘ (ìŠ¤í¬ë˜í•‘)")
    all_articles = []
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    for keyword in keywords:
        try:
            encoded = urllib.parse.quote(keyword)
            url = f"https://search.naver.com/search.naver?where=news&query={encoded}&sort=1"
            
            logger.debug(f"   ê²€ìƒ‰ì–´: {keyword}")
            response = requests.get(url, headers=headers, timeout=config.API_TIMEOUT)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            selectors = [
                'a.news_tit',
                '.news_area .news_tit',
                'div.news_wrap a.news_tit',
                'a[href*="n.news.naver.com"]'
            ]
            
            items = []
            for selector in selectors:
                items = soup.select(selector)
                if items:
                    logger.debug(f"   ì…€ë ‰í„° '{selector}' ì‚¬ìš©")
                    break
            
            if not items:
                logger.warning(f"   âš ï¸ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. HTML êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                with open(f'/home/claude/naver_debug_{keyword}.html', 'w', encoding='utf-8') as f:
                    f.write(response.text[:5000])
                logger.debug(f"   ë””ë²„ê·¸ HTML ì €ì¥: naver_debug_{keyword}.html")
                continue
            
            logger.debug(f"   ë°œê²¬: {len(items)}ê°œ")
            
            for item in items[:20]:
                try:
                    title = item.get_text(strip=True)
                    link = item.get('href', '')
                    
                    if not link or not title:
                        continue
                    
                    all_articles.append({
                        "title": title,
                        "link": link,
                        "description": "",
                        "published_at": datetime.now(timezone.utc).isoformat(),
                        "source": "Naver News",
                        "country": "Korea"
                    })
                except Exception as e:
                    logger.debug(f"   í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue
            
            time.sleep(1)
            
        except requests.exceptions.Timeout:
            logger.error(f"   âŒ íƒ€ì„ì•„ì›ƒ (30ì´ˆ)")
            continue
        except Exception as e:
            logger.error(f"   âŒ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            continue
    
    logger.info(f"   âœ… Naver News: {len(all_articles)}ê°œ ìˆ˜ì§‘")
    return all_articles
