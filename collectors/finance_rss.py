"""
ê²½ì œ/ê¸ˆìœµ ì „ë¬¸ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°
WSJ, BBC Business, NPR ë“± ê²½ì œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""
import time
import urllib.parse
import requests
from typing import List, Dict
import feedparser
from datetime import datetime, timezone
import config
import collector_utils as utils

logger = config.setup_logger(__name__)

# User-Agent í—¤ë” (RSS ì°¨ë‹¨ ë°©ì§€)
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}


def _fetch_feed(url: str) -> feedparser.FeedParserDict:
    """RSS í”¼ë“œë¥¼ User-Agent í—¤ë”ì™€ í•¨ê»˜ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        response = requests.get(url, headers=HEADERS, timeout=15)
        response.raise_for_status()
        return feedparser.parse(response.content)
    except requests.RequestException as e:
        logger.debug(f"   ìš”ì²­ ì‹¤íŒ¨, feedparserë¡œ ì¬ì‹œë„: {e}")
        return feedparser.parse(url)


def fetch_finance_rss() -> List[Dict]:
    """
    ê²½ì œ/ê¸ˆìœµ ì „ë¬¸ RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    í•œêµ­ ì‹œì¥ ì˜í–¥ ë¶„ì„ìš© í‚¤ì›Œë“œë¡œ í•„í„°ë§í•©ë‹ˆë‹¤.
    """
    logger.info("ğŸ“¡ [Finance RSS] ìˆ˜ì§‘ ì‹œì‘")
    all_articles = []
    
    # ê²½ì œ/ê¸ˆìœµ í‚¤ì›Œë“œë¡œ í•„í„°ë§
    keywords_lower = [k.lower() for k in config.KEYWORDS_FINANCE_EN]
    
    for source in config.FINANCE_RSS_SOURCES:
        try:
            url = source['url']
            name = source['name']
            country = source.get('country', 'Global')
            category = source.get('category', 'finance')
            
            logger.debug(f"   {name}: {url}")
            feed = _fetch_feed(url)
            
            # í”¼ë“œ ìƒíƒœ í™•ì¸
            if hasattr(feed, 'bozo') and feed.bozo:
                logger.warning(f"   âš ï¸ {name}: í”¼ë“œ íŒŒì‹± ë¬¸ì œ ë°œìƒ")
            
            filtered_count = 0
            for entry in feed.entries[:50]:  # ì†ŒìŠ¤ë‹¹ ìµœëŒ€ 50ê°œ
                try:
                    title = getattr(entry, 'title', '')
                    link = getattr(entry, 'link', '')
                    
                    if not title or not link:
                        continue
                    
                    title_lower = title.lower()
                    summary = getattr(entry, 'summary', '') or ''
                    summary_lower = summary.lower()
                    
                    # í•œêµ­ ì‹œì¥ ê´€ë ¨ í‚¤ì›Œë“œ í•„í„°ë§
                    if any(kw in title_lower or kw in summary_lower for kw in keywords_lower):
                        all_articles.append({
                            "title": title,
                            "link": link,
                            "description": summary,
                            "published_at": utils.clean_date(getattr(entry, 'published', '')),
                            "source": name,
                            "country": country,
                            "category": category
                        })
                        filtered_count += 1
                except Exception as e:
                    logger.debug(f"   í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue
            
            logger.debug(f"   {name}: {len(feed.entries)}ê°œ ì¤‘ {filtered_count}ê°œ ê´€ë ¨")
            time.sleep(0.5)  # Rate limit ë°©ì§€
            
        except Exception as e:
            logger.error(f"   âŒ {source.get('name', 'Unknown')} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            continue
    
    logger.info(f"   âœ… Finance RSS: {len(all_articles)}ê°œ ìˆ˜ì§‘")
    return all_articles


def fetch_finance_rss_all() -> List[Dict]:
    """
    ê²½ì œ/ê¸ˆìœµ ì „ë¬¸ RSSì—ì„œ ëª¨ë“  ë‰´ìŠ¤ ìˆ˜ì§‘ (í•„í„°ë§ ì—†ì´).
    AI ë¶„ì„ ë‹¨ê³„ì—ì„œ í•œêµ­ ì‹œì¥ ì˜í–¥ì„ íŒë‹¨í•©ë‹ˆë‹¤.
    """
    logger.info("ğŸ“¡ [Finance RSS ALL] ìˆ˜ì§‘ ì‹œì‘")
    all_articles = []
    
    for source in config.FINANCE_RSS_SOURCES:
        try:
            url = source['url']
            name = source['name']
            country = source.get('country', 'Global')
            category = source.get('category', 'finance')
            
            logger.debug(f"   {name}: {url}")
            feed = _fetch_feed(url)
            
            if hasattr(feed, 'bozo') and feed.bozo:
                logger.warning(f"   âš ï¸ {name}: í”¼ë“œ íŒŒì‹± ë¬¸ì œ ë°œìƒ")
            
            for entry in feed.entries[:30]:  # ì†ŒìŠ¤ë‹¹ ìµœëŒ€ 30ê°œ (ì „ì²´ ìˆ˜ì§‘ì´ë¯€ë¡œ ì œí•œ)
                try:
                    title = getattr(entry, 'title', '')
                    link = getattr(entry, 'link', '')
                    
                    if not title or not link:
                        continue
                    
                    all_articles.append({
                        "title": title,
                        "link": link,
                        "description": getattr(entry, 'summary', '') or '',
                        "published_at": utils.clean_date(getattr(entry, 'published', '')),
                        "source": name,
                        "country": country,
                        "category": category
                    })
                except Exception as e:
                    logger.debug(f"   í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue
            
            logger.debug(f"   {name}: {len(feed.entries)}ê°œ ìˆ˜ì§‘")
            time.sleep(0.5)
            
        except Exception as e:
            logger.error(f"   âŒ {source.get('name', 'Unknown')} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            continue
    
    logger.info(f"   âœ… Finance RSS ALL: {len(all_articles)}ê°œ ìˆ˜ì§‘")
    return all_articles


def detect_affected_sectors(title: str, description: str = "") -> List[str]:
    """
    ë‰´ìŠ¤ ì œëª©ê³¼ ë‚´ìš©ì—ì„œ ì˜í–¥ë°›ëŠ” í•œêµ­ ì‹œì¥ ì„¹í„°ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
    
    Args:
        title: ë‰´ìŠ¤ ì œëª©
        description: ë‰´ìŠ¤ ë‚´ìš©/ìš”ì•½
        
    Returns:
        List[str]: ì˜í–¥ë°›ëŠ” ì„¹í„° ëª©ë¡
    """
    text = f"{title} {description}".lower()
    affected_sectors = []
    
    for sector, keywords in config.SECTOR_KEYWORDS.items():
        for keyword in keywords:
            if keyword.lower() in text:
                if sector not in affected_sectors:
                    affected_sectors.append(sector)
                break  # í•˜ë‚˜ë§Œ ë§¤ì¹­ë˜ë©´ í•´ë‹¹ ì„¹í„° ì¶”ê°€
    
    return affected_sectors


def calculate_korea_relevance(title: str, description: str = "") -> str:
    """
    ë‰´ìŠ¤ì˜ í•œêµ­ ì‹œì¥ ê´€ë ¨ì„±ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Returns:
        str: 'high', 'medium', 'low', 'none'
    """
    text = f"{title} {description}".lower()
    
    # ì§ì ‘ ê´€ë ¨ í‚¤ì›Œë“œ
    direct_keywords = [
        "korea", "korean", "samsung", "sk hynix", "hyundai", "kia",
        "lg", "kospi", "kosdaq", "won", "krw", "seoul"
    ]
    
    # ê°„ì ‘ ê´€ë ¨ í‚¤ì›Œë“œ (ê¸€ë¡œë²Œ ì˜í–¥)
    indirect_keywords = [
        "semiconductor", "chip", "battery", "ev", "fed", "interest rate",
        "china", "japan", "trade", "tariff", "oil", "nvidia", "tsmc"
    ]
    
    direct_count = sum(1 for kw in direct_keywords if kw in text)
    indirect_count = sum(1 for kw in indirect_keywords if kw in text)
    
    if direct_count >= 2:
        return "high"
    elif direct_count >= 1:
        return "medium"
    elif indirect_count >= 3:
        return "medium"
    elif indirect_count >= 1:
        return "low"
    else:
        return "none"


# í…ŒìŠ¤íŠ¸ìš©
if __name__ == "__main__":
    print("ê²½ì œ ë‰´ìŠ¤ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸...")
    articles = fetch_finance_rss()
    print(f"\nìˆ˜ì§‘ëœ ê¸°ì‚¬: {len(articles)}ê°œ")
    
    for article in articles[:5]:
        sectors = detect_affected_sectors(article['title'], article['description'])
        relevance = calculate_korea_relevance(article['title'], article['description'])
        print(f"\nì œëª©: {article['title'][:60]}...")
        print(f"ì†ŒìŠ¤: {article['source']}")
        print(f"í•œêµ­ ê´€ë ¨ì„±: {relevance}")
        print(f"ì˜í–¥ ì„¹í„°: {sectors}")
