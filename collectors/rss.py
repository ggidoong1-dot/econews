import time
import urllib.parse
from typing import List, Dict
import feedparser
from datetime import datetime, timezone
import config
import collector_utils as utils

logger = config.setup_logger(__name__)


def fetch_google_news(keywords: List[str]) -> List[Dict]:
    """
    Google News RSSì—ì„œ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    logger.info("ğŸ“¡ [Google News] ìˆ˜ì§‘ ì‹œì‘")
    all_articles: List[Dict] = []

    for keyword in keywords:
        try:
            encoded = urllib.parse.quote(keyword)
            url = f"https://news.google.com/rss/search?q={encoded}&hl=en-US&gl=US&ceid=US:en"

            logger.debug(f"   ê²€ìƒ‰ì–´: {keyword}")
            feed = feedparser.parse(url)

            # í”¼ë“œ ìƒíƒœ í™•ì¸
            if hasattr(feed, 'status') and feed.status != 200:
                logger.warning(f"   âš ï¸ HTTP {feed.status}")
                continue

            logger.debug(f"   ë°œê²¬: {len(feed.entries)}ê°œ")

            # ìµœëŒ€ 20ê°œ í•­ëª©ë§Œ ì²˜ë¦¬
            for entry in feed.entries[:20]:
                try:
                    title = entry.get('title', '') if hasattr(entry, 'get') else getattr(entry, 'title', '')
                    link = entry.get('link', '') if hasattr(entry, 'get') else getattr(entry, 'link', '')
                    if not title or not link:
                        continue

                    published = utils.clean_date(entry.get('published', '') if hasattr(entry, 'get') else getattr(entry, 'published', ''))
                    description = entry.get('summary', '') if hasattr(entry, 'get') else getattr(entry, 'summary', '')

                    all_articles.append({
                        "title": title,
                        "link": link,
                        "description": description,
                        "published_at": published,
                        "source": "Google News",
                        "country": "Global"
                    })
                except Exception as e:
                    logger.debug(f"   í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue

            time.sleep(1)

        except Exception as e:
            logger.error(f"   âŒ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            continue

    logger.info(f"   âœ… Google News: {len(all_articles)}ê°œ ìˆ˜ì§‘")
    return all_articles


def fetch_reddit_rss(subreddit_urls: List[str]) -> List[Dict]:
    """Reddit RSS í”¼ë“œì—ì„œ ê²Œì‹œë¬¼ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    logger.info("ğŸ“¡ [Reddit] ìˆ˜ì§‘ ì‹œì‘")
    all_articles = []
    
    for url in subreddit_urls:
        try:
            logger.debug(f"   URL: {url}")
            feed = feedparser.parse(url)
            
            logger.debug(f"   ë°œê²¬: {len(feed.entries)}ê°œ")
            
            for entry in feed.entries:
                all_articles.append({
                    "title": entry.title,
                    "link": entry.link,
                    "description": entry.get('summary', ''),
                    "published_at": utils.clean_date(entry.get('published', '')),
                    "source": "Reddit",
                    "country": "Global"
                })
            
            time.sleep(1)
            
        except Exception as e:
            logger.error(f"   âŒ {url} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            continue
    
    logger.info(f"   âœ… Reddit: {len(all_articles)}ê°œ ìˆ˜ì§‘")
    return all_articles


def fetch_direct_rss(rss_sources: List[Dict], keywords: List[str] = None) -> List[Dict]:
    """ì§ì ‘ ë‰´ìŠ¤ì‚¬ì´íŠ¸ RSSë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    logger.info("ğŸ“¡ [Direct RSS] ìˆ˜ì§‘ ì‹œì‘")
    all_articles = []
    
    # í‚¤ì›Œë“œê°€ ì „ë‹¬ë˜ì§€ ì•Šìœ¼ë©´ config ê¸°ë³¸ê°’ ì‚¬ìš©
    if keywords is None:
        keywords = config.KEYWORDS_EN
    
    for source in rss_sources:
        try:
            url = source['url']
            name = source['name']
            country = source.get('country', 'Global')
            
            logger.debug(f"   {name}: {url}")
            feed = feedparser.parse(url)
            
            # í‚¤ì›Œë“œ í•„í„°ë§ (ëª¨ë“  ê¸°ì‚¬ê°€ ì•„ë‹Œ ê´€ë ¨ ê¸°ì‚¬ë§Œ)
            keywords_lower = [k.lower() for k in keywords]
            
            filtered_count = 0
            for entry in feed.entries:
                title = entry.title.lower()
                summary = entry.get('summary', '').lower()
                
                # í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸
                if any(keyword in title or keyword in summary for keyword in keywords_lower):
                    all_articles.append({
                        "title": entry.title,
                        "link": entry.link,
                        "description": entry.get('summary', ''),
                        "published_at": utils.clean_date(entry.get('published', '')),
                        "source": name,
                        "country": country
                    })
                    filtered_count += 1
            
            logger.debug(f"   ë°œê²¬: {len(feed.entries)}ê°œ ì¤‘ {filtered_count}ê°œ ê´€ë ¨")
            time.sleep(1)
            
        except Exception as e:
            logger.error(f"   âŒ {source.get('name', 'Unknown')} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            continue
    
    logger.info(f"   âœ… Direct RSS: {len(all_articles)}ê°œ ìˆ˜ì§‘")
    return all_articles


def fetch_bing_news(keywords: List[str]) -> List[Dict]:
    """Bing Newsìš© ì„ì‹œ ìŠ¤í…. ì‹¤ì œ êµ¬í˜„ì´ ì—†ì„ ë•Œ ì˜¤ë¥˜ë¥¼ í”¼í•˜ê¸° ìœ„í•´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜."""
    logger.warning("Bing News ìˆ˜ì§‘ê¸°ëŠ” ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¹ˆ ê²°ê³¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
    return []


def fetch_newsapi(keywords: List[str]) -> List[Dict]:
    """NewsAPIì—ì„œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    import requests
    
    api_key = config.NEWSAPI_KEY
    if not api_key:
        logger.warning("   âš ï¸ NEWSAPI_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
        return []
    
    logger.info("ğŸ“¡ [NewsAPI] ìˆ˜ì§‘ ì‹œì‘")
    all_articles = []
    
    for keyword in keywords:
        try:
            encoded = urllib.parse.quote(keyword)
            url = f"https://newsapi.org/v2/everything?q={encoded}&language=en&sortBy=publishedAt&pageSize=20&apiKey={api_key}"
            
            logger.debug(f"   ê²€ìƒ‰ì–´: {keyword}")
            response = requests.get(url, timeout=config.API_TIMEOUT)
            response.raise_for_status()
            
            data = response.json()
            
            if data.get('status') != 'ok':
                logger.warning(f"   âš ï¸ API ì‘ë‹µ ì˜¤ë¥˜: {data.get('message', 'Unknown error')}")
                continue
            
            articles = data.get('articles', [])
            logger.debug(f"   ë°œê²¬: {len(articles)}ê°œ")
            
            for article in articles[:20]:
                try:
                    title = article.get('title', '')
                    link = article.get('url', '')
                    
                    if not title or not link or title == '[Removed]':
                        continue
                    
                    # ë‚ ì§œ ì²˜ë¦¬
                    pub_date = article.get('publishedAt', '')
                    if pub_date:
                        pub_date = utils.clean_date(pub_date)
                    else:
                        pub_date = datetime.now(timezone.utc).isoformat()
                    
                    all_articles.append({
                        "title": title,
                        "link": link,
                        "description": article.get('description', '') or '',
                        "published_at": pub_date,
                        "source": f"NewsAPI/{article.get('source', {}).get('name', 'Unknown')}",
                        "country": "Global"
                    })
                except Exception as e:
                    logger.debug(f"   í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue
            
            time.sleep(1)  # Rate limit ë°©ì§€
            
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 426:
                logger.error("   âŒ NewsAPI ë¬´ë£Œ í”Œëœì€ HTTPSë§Œ ì§€ì›í•©ë‹ˆë‹¤. ìœ ë£Œ í”Œëœì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            elif e.response.status_code == 401:
                logger.error("   âŒ NewsAPI í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            elif e.response.status_code == 429:
                logger.error("   âŒ NewsAPI ìš”ì²­ í•œë„ ì´ˆê³¼. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
            else:
                logger.error(f"   âŒ HTTP ì˜¤ë¥˜: {e}")
            break  # API ì˜¤ë¥˜ ì‹œ ë” ì´ìƒ ì‹œë„í•˜ì§€ ì•ŠìŒ
        except Exception as e:
            logger.error(f"   âŒ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            continue
    
    logger.info(f"   âœ… NewsAPI: {len(all_articles)}ê°œ ìˆ˜ì§‘")
    return all_articles
