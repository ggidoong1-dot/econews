"""
ìƒˆë²½ ì „ë¬¸ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°
í•œêµ­ ì‹œê°„ 0ì‹œ~7ì‹œ ì‚¬ì´ ë°œìƒí•˜ëŠ” ê¸€ë¡œë²Œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

ì£¼ìš” ìˆ˜ì§‘ ëŒ€ìƒ:
- ë¯¸êµ­ ì¥ ë§ˆê° ë‰´ìŠ¤ (í•œêµ­ì‹œê°„ 6ì‹œ~7ì‹œ)
- ìœ ëŸ½ ê²½ì œ ë‰´ìŠ¤ (í•œêµ­ì‹œê°„ 0ì‹œ~4ì‹œ)
- ì•„ì‹œì•„ ì„ ë¬¼ ì‹œì¥ ë™í–¥ (í•œêµ­ì‹œê°„ 5ì‹œ~7ì‹œ)
"""
import time
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Optional
import feedparser
import requests

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import config

logger = config.setup_logger(__name__)

# User-Agent í—¤ë” (RSS ì°¨ë‹¨ ë°©ì§€)
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

# ==============================================
# ìƒˆë²½ ì‹œê°„ëŒ€ ì „ìš© ë‰´ìŠ¤ ì†ŒìŠ¤
# ==============================================
NIGHT_RSS_SOURCES = [
    # ============ ë¯¸êµ­ ë©”ì¸ ë‰´ìŠ¤ ============
    {
        "url": "https://feeds.a.dj.com/rss/RSSMarketsMain.xml",
        "name": "WSJ Markets",
        "country": "US",
        "category": "markets",
        "priority": "high"
    },
    {
        "url": "https://feeds.a.dj.com/rss/WSJcomUSBusiness.xml",
        "name": "WSJ Business",
        "country": "US",
        "category": "finance",
        "priority": "high"
    },
    {
        "url": "https://feeds.a.dj.com/rss/RSSWorldNews.xml",
        "name": "WSJ World",
        "country": "US",
        "category": "world",
        "priority": "medium"
    },
    # ============ CNBC (ë¯¸êµ­ ì¦ì‹œ í•µì‹¬) ============
    {
        "url": "https://www.cnbc.com/id/100003114/device/rss/rss.html",
        "name": "CNBC Top News",
        "country": "US",
        "category": "markets",
        "priority": "high"
    },
    {
        "url": "https://www.cnbc.com/id/20910258/device/rss/rss.html",
        "name": "CNBC Pre-Markets",
        "country": "US",
        "category": "markets",
        "priority": "high"
    },
    {
        "url": "https://www.cnbc.com/id/10000664/device/rss/rss.html",
        "name": "CNBC US Markets",
        "country": "US",
        "category": "markets",
        "priority": "high"
    },
    # ============ Reuters ============
    {
        "url": "https://www.reutersagency.com/feed/?best-topics=business-finance&post_type=best",
        "name": "Reuters Business",
        "country": "Global",
        "category": "finance",
        "priority": "high"
    },
    # ============ BBC/NPR ============
    {
        "url": "https://feeds.bbci.co.uk/news/business/rss.xml",
        "name": "BBC Business",
        "country": "UK",
        "category": "finance",
        "priority": "medium"
    },
    {
        "url": "https://feeds.npr.org/1006/rss.xml",
        "name": "NPR Business",
        "country": "US",
        "category": "finance",
        "priority": "medium"
    },
    # ============ í…Œí¬/ë°˜ë„ì²´ ============
    {
        "url": "https://feeds.bbci.co.uk/news/technology/rss.xml",
        "name": "BBC Technology",
        "country": "UK",
        "category": "tech",
        "priority": "medium"
    },
    {
        "url": "https://www.theverge.com/rss/index.xml",
        "name": "The Verge",
        "country": "US",
        "category": "tech",
        "priority": "low"
    },
    # ============ ì•„ì‹œì•„ ë‰´ìŠ¤ ============
    {
        "url": "https://www3.nhk.or.jp/rss/news/cat5.xml",
        "name": "NHK Business",
        "country": "Japan",
        "category": "finance",
        "priority": "medium"
    },
    {
        "url": "https://www.scmp.com/rss/91/feed",
        "name": "SCMP Business",
        "country": "China/HK",
        "category": "finance",
        "priority": "medium"
    },
    # ============ ì›ìì¬/ì—ë„ˆì§€ ============
    {
        "url": "https://oilprice.com/rss/main",
        "name": "OilPrice",
        "country": "Global",
        "category": "commodities",
        "priority": "medium"
    },
]

# í•œêµ­ ì‹œì¥ ì˜í–¥ í‚¤ì›Œë“œ (ë¹ ë¥¸ í•„í„°ë§ìš©)
KOREA_IMPACT_KEYWORDS = [
    # ì§ì ‘ ê´€ë ¨
    "korea", "korean", "samsung", "sk hynix", "hyundai", "kia", "lg",
    "kospi", "kosdaq", "won", "krw", "seoul",
    # ë°˜ë„ì²´
    "semiconductor", "chip", "nvidia", "tsmc", "intel", "amd", 
    "memory", "dram", "nand", "hbm", "ai chip",
    # 2ì°¨ì „ì§€/EV
    "battery", "ev", "electric vehicle", "tesla", "lithium",
    # í†µí™”/ê¸ˆë¦¬
    "fed", "federal reserve", "interest rate", "inflation",
    "fomc", "powell", "rate cut", "rate hike",
    # ë¹…í…Œí¬
    "apple", "google", "microsoft", "amazon", "meta", "nvidia",
    # ë¬´ì—­/ì§€ì •í•™
    "china", "trade war", "tariff", "sanction", "export",
    # ì›ìì¬
    "oil", "crude", "gold", "copper",
    # ì‹œì¥ ì§€í‘œ
    "s&p", "nasdaq", "dow", "treasury", "yield", "vix",
]


def _fetch_feed(url: str, timeout: int = 15) -> feedparser.FeedParserDict:
    """RSS í”¼ë“œë¥¼ User-Agent í—¤ë”ì™€ í•¨ê»˜ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        response = requests.get(url, headers=HEADERS, timeout=timeout)
        response.raise_for_status()
        return feedparser.parse(response.content)
    except requests.RequestException as e:
        logger.debug(f"   ìš”ì²­ ì‹¤íŒ¨, feedparserë¡œ ì¬ì‹œë„: {e}")
        return feedparser.parse(url)


def _is_night_time_kst() -> bool:
    """í˜„ì¬ ì‹œê°„ì´ í•œêµ­ ê¸°ì¤€ ìƒˆë²½ ì‹œê°„ëŒ€(0ì‹œ~7ì‹œ)ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    kst = timezone(timedelta(hours=9))
    now_kst = datetime.now(kst)
    return 0 <= now_kst.hour < 7


def _get_hours_ago(hours: int) -> datetime:
    """ì§€ì •ëœ ì‹œê°„ ì „ì˜ datetime ë°˜í™˜"""
    return datetime.now(timezone.utc) - timedelta(hours=hours)


def _quick_filter_relevant(title: str, description: str) -> bool:
    """í•œêµ­ ì‹œì¥ ê´€ë ¨ ë‰´ìŠ¤ì¸ì§€ ë¹ ë¥´ê²Œ í•„í„°ë§ (í‚¤ì›Œë“œ ê¸°ë°˜)"""
    text = f"{title} {description}".lower()
    return any(kw in text for kw in KOREA_IMPACT_KEYWORDS)


def collect_night_news(
    hours_back: int = 8,
    filter_korea_relevant: bool = True,
    priority_filter: Optional[List[str]] = None
) -> List[Dict]:
    """
    ìƒˆë²½ ì‹œê°„ëŒ€ ê¸€ë¡œë²Œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    
    Args:
        hours_back: ëª‡ ì‹œê°„ ì „ê¹Œì§€ ìˆ˜ì§‘í• ì§€ (ê¸°ë³¸ 8ì‹œê°„)
        filter_korea_relevant: í•œêµ­ ì‹œì¥ ê´€ë ¨ ë‰´ìŠ¤ë§Œ í•„í„°ë§í• ì§€
        priority_filter: ìˆ˜ì§‘í•  ìš°ì„ ìˆœìœ„ ëª©ë¡ (ì˜ˆ: ["high", "medium"])
        
    Returns:
        List[Dict]: ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ëª©ë¡
    """
    logger.info("=" * 60)
    logger.info("ğŸŒ™ ìƒˆë²½ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
    logger.info(f"   ìˆ˜ì§‘ ë²”ìœ„: ìµœê·¼ {hours_back}ì‹œê°„")
    logger.info(f"   í•œêµ­ ê´€ë ¨ í•„í„°: {'ON' if filter_korea_relevant else 'OFF'}")
    logger.info("=" * 60)
    
    if priority_filter is None:
        priority_filter = ["high", "medium", "low"]
    
    all_articles = []
    cutoff_time = _get_hours_ago(hours_back)
    
    # í™œì„± ì†ŒìŠ¤ í•„í„°ë§
    active_sources = [
        s for s in NIGHT_RSS_SOURCES 
        if s.get("priority", "low") in priority_filter
    ]
    
    logger.info(f"ğŸ“¡ í™œì„± ë‰´ìŠ¤ ì†ŒìŠ¤: {len(active_sources)}ê°œ")
    
    for source in active_sources:
        try:
            url = source['url']
            name = source['name']
            country = source.get('country', 'Global')
            category = source.get('category', 'general')
            priority = source.get('priority', 'low')
            
            logger.debug(f"   [{priority.upper()}] {name}: {url}")
            feed = _fetch_feed(url)
            
            # í”¼ë“œ ìƒíƒœ í™•ì¸
            if hasattr(feed, 'bozo') and feed.bozo:
                logger.warning(f"   âš ï¸ {name}: í”¼ë“œ íŒŒì‹± ë¬¸ì œ ë°œìƒ")
            
            source_count = 0
            for entry in feed.entries[:40]:  # ì†ŒìŠ¤ë‹¹ ìµœëŒ€ 40ê°œ
                try:
                    title = getattr(entry, 'title', '')
                    link = getattr(entry, 'link', '')
                    
                    if not title or not link:
                        continue
                    
                    # ë°œí–‰ì¼ íŒŒì‹± (ì—†ìœ¼ë©´ í˜„ì¬ ì‹œê°„ìœ¼ë¡œ)
                    published = getattr(entry, 'published_parsed', None)
                    if published:
                        pub_datetime = datetime(*published[:6], tzinfo=timezone.utc)
                        # ì‹œê°„ ë²”ìœ„ ì²´í¬
                        if pub_datetime < cutoff_time:
                            continue
                    else:
                        pub_datetime = datetime.now(timezone.utc)
                    
                    summary = getattr(entry, 'summary', '') or ''
                    
                    # í•œêµ­ ê´€ë ¨ ë¹ ë¥¸ í•„í„°ë§
                    if filter_korea_relevant:
                        if not _quick_filter_relevant(title, summary):
                            continue
                    
                    article = {
                        "title": title,
                        "link": link,
                        "description": summary[:500],  # ê¸¸ì´ ì œí•œ
                        "published_at": pub_datetime.isoformat(),
                        "source": name,
                        "country": country,
                        "category": category,
                        "priority": priority,
                        "collected_at": datetime.now(timezone.utc).isoformat()
                    }
                    
                    all_articles.append(article)
                    source_count += 1
                    
                except Exception as e:
                    logger.debug(f"   í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue
            
            if source_count > 0:
                logger.info(f"   âœ… {name}: {source_count}ê°œ ìˆ˜ì§‘")
            
            time.sleep(0.5)  # Rate limit ë°©ì§€
            
        except Exception as e:
            logger.error(f"   âŒ {source.get('name', 'Unknown')} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            continue
    
    # ì¤‘ë³µ ì œê±° (ë§í¬ ê¸°ì¤€)
    seen_links = set()
    unique_articles = []
    for article in all_articles:
        if article['link'] not in seen_links:
            seen_links.add(article['link'])
            unique_articles.append(article)
    
    # ë°œí–‰ì¼ìˆœ ì •ë ¬ (ìµœì‹  ë¨¼ì €)
    unique_articles.sort(key=lambda x: x.get('published_at', ''), reverse=True)
    
    logger.info("=" * 60)
    logger.info(f"ğŸŒ™ ìƒˆë²½ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(unique_articles)}ê°œ")
    logger.info("=" * 60)
    
    return unique_articles


def collect_priority_news(hours_back: int = 4) -> List[Dict]:
    """
    ìš°ì„ ìˆœìœ„ ë†’ì€ ë‰´ìŠ¤ë§Œ ë¹ ë¥´ê²Œ ìˆ˜ì§‘ (ê¸´ê¸‰ ì•Œë¦¼ìš©)
    
    Args:
        hours_back: ëª‡ ì‹œê°„ ì „ê¹Œì§€ ìˆ˜ì§‘í• ì§€
        
    Returns:
        List[Dict]: ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ëª©ë¡
    """
    return collect_night_news(
        hours_back=hours_back,
        filter_korea_relevant=True,
        priority_filter=["high"]
    )


def get_night_summary_stats(articles: List[Dict]) -> Dict:
    """
    ìƒˆë²½ ë‰´ìŠ¤ ìˆ˜ì§‘ í†µê³„ ìš”ì•½
    
    Args:
        articles: ìˆ˜ì§‘ëœ ê¸°ì‚¬ ëª©ë¡
        
    Returns:
        Dict: í†µê³„ ìš”ì•½
    """
    if not articles:
        return {"total": 0, "by_country": {}, "by_category": {}, "by_priority": {}}
    
    stats = {
        "total": len(articles),
        "by_country": {},
        "by_category": {},
        "by_priority": {},
        "time_range": {
            "earliest": None,
            "latest": None
        }
    }
    
    for article in articles:
        # êµ­ê°€ë³„
        country = article.get('country', 'Unknown')
        stats["by_country"][country] = stats["by_country"].get(country, 0) + 1
        
        # ì¹´í…Œê³ ë¦¬ë³„
        category = article.get('category', 'general')
        stats["by_category"][category] = stats["by_category"].get(category, 0) + 1
        
        # ìš°ì„ ìˆœìœ„ë³„
        priority = article.get('priority', 'low')
        stats["by_priority"][priority] = stats["by_priority"].get(priority, 0) + 1
        
        # ì‹œê°„ ë²”ìœ„
        pub_at = article.get('published_at', '')
        if pub_at:
            if stats["time_range"]["earliest"] is None or pub_at < stats["time_range"]["earliest"]:
                stats["time_range"]["earliest"] = pub_at
            if stats["time_range"]["latest"] is None or pub_at > stats["time_range"]["latest"]:
                stats["time_range"]["latest"] = pub_at
    
    return stats


# ==============================================
# í…ŒìŠ¤íŠ¸
# ==============================================
if __name__ == "__main__":
    print("=" * 60)
    print("ìƒˆë²½ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # í•œêµ­ ì‹œê°„ëŒ€ í™•ì¸
    kst = timezone(timedelta(hours=9))
    now_kst = datetime.now(kst)
    print(f"\ní˜„ì¬ ì‹œê°„ (KST): {now_kst.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ìƒˆë²½ ì‹œê°„ëŒ€ ì—¬ë¶€: {_is_night_time_kst()}")
    
    # ë‰´ìŠ¤ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
    print("\nğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ (ìµœê·¼ 4ì‹œê°„, í•œêµ­ ê´€ë ¨ë§Œ)...")
    articles = collect_night_news(hours_back=4, filter_korea_relevant=True)
    
    # í†µê³„
    stats = get_night_summary_stats(articles)
    print(f"\nğŸ“Š ìˆ˜ì§‘ í†µê³„:")
    print(f"   ì´ ê¸°ì‚¬: {stats['total']}ê°œ")
    print(f"   êµ­ê°€ë³„: {stats['by_country']}")
    print(f"   ì¹´í…Œê³ ë¦¬ë³„: {stats['by_category']}")
    print(f"   ìš°ì„ ìˆœìœ„ë³„: {stats['by_priority']}")
    
    # ìƒìœ„ 5ê°œ ê¸°ì‚¬ ì¶œë ¥
    print("\nğŸ“° ìˆ˜ì§‘ëœ ê¸°ì‚¬ (ìƒìœ„ 5ê°œ):")
    for i, article in enumerate(articles[:5], 1):
        print(f"\n{i}. [{article['source']}] {article['title'][:60]}...")
        print(f"   ë°œí–‰: {article.get('published_at', 'N/A')[:19]}")
